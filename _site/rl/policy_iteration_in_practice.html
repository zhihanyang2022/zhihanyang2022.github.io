<html>
<head>
	<meta charset="UTF-8">
	<title>Policy Iteration in Practice</title>
	<link rel="stylesheet" type="text/css" href="/assets/rl_post.css">
	<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
</head>
<body>
	<div class="content">
		<p><a href="/rl">Back</a> to the list of RL posts.</a>
		<h1>Policy Iteration in Practice</h1>
		<p>By Zhihan on 2020-07-21 11:30:00 -0500.</p>
		<p>Please note that this webpage does not scale well on mobile devices.</p>
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#book-yes-but-use-either-discounting-or-non-deterministic-policy" id="markdown-toc-book-yes-but-use-either-discounting-or-non-deterministic-policy">Book: Yes, but use either discounting or non-deterministic policy.</a>    <ul>
      <li><a href="#environment" id="markdown-toc-environment">Environment</a></li>
      <li><a href="#initial-q-table" id="markdown-toc-initial-q-table">Initial q-table</a></li>
      <li><a href="#policy-evaluation-update-formula" id="markdown-toc-policy-evaluation-update-formula">Policy-evaluation update formula</a>        <ul>
          <li><a href="#no-discounting-and-deterministic-policy" id="markdown-toc-no-discounting-and-deterministic-policy">No discounting and deterministic policy</a></li>
          <li><a href="#discounting-and-deterministic-policy" id="markdown-toc-discounting-and-deterministic-policy">Discounting and deterministic policy</a></li>
          <li><a href="#no-discounting-and-epsilon-greedy-policy" id="markdown-toc-no-discounting-and-epsilon-greedy-policy">No discounting and epsilon-greedy policy</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#me-i-would-like-to-learn-a-deterministic-policy-without-using-discounting" id="markdown-toc-me-i-would-like-to-learn-a-deterministic-policy-without-using-discounting">Me: I would like to learn a deterministic policy without using discounting.</a>    <ul>
      <li><a href="#approach-1-decay-epsilon-of-the-epsilon-greedy-policy" id="markdown-toc-approach-1-decay-epsilon-of-the-epsilon-greedy-policy">Approach 1: Decay epsilon of the epsilon-greedy policy</a></li>
      <li><a href="#approach-2-truncate-policy-evaluation" id="markdown-toc-approach-2-truncate-policy-evaluation">Approach 2: Truncate policy evaluation</a></li>
    </ul>
  </li>
  <li><a href="#gallery" id="markdown-toc-gallery">Gallery</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Algorithms introduced in Chapter 5 (Monte-Carlo methods) and Chapter 6 (TD methods) of Sutton &amp; Barto 2018 approximate the optimal q-table to achieve control. Since these methods were a bit hard to debug and only converge in the limit, I needed a tool to compute the exact optimal q-table to evaluate their performance. Therefore, I went back to Chapter 4 (dynamic programming) and implemented the policy iteration algorithm using q-tables with this question in mind:</p>

<blockquote>
  <p>Can policy iteration be used to learn optimal deterministic policies and their corresponding optimal value functions?</p>
</blockquote>

<p>I also show some galleries of learned value functions in the end along with their source code.</p>

<h2 id="book-yes-but-use-either-discounting-or-non-deterministic-policy">Book: Yes, but use either discounting or non-deterministic policy.</h2>

<p>In many problems, our goal is to learn the optimal <strong>deterministic</strong> policy. This is because deterministic policies perform optimally <strong>all the time</strong> and thus obtain the highest reward.</p>

<p>Policy iteration is an algorithm that allows us to find the optimal policy and the optimal value function in the limit. It consists of two sub-algorithms: policy evaluation and policy improvement. Certain requirements need to be met for policy evaluation to work. Here’s a relevant quote from section 4.1 of Sutton &amp; Barto 2018:</p>

<blockquote>
  <p>The existence and uniqueness of <script type="math/tex">v_{\pi}</script> are guaranteed as long as either <script type="math/tex">% <![CDATA[
\gamma < 1 %]]></script> or eventual termination is guaranteed from all states under the policy <script type="math/tex">\pi</script>.</p>
</blockquote>

<p>To understand this quote, let’s consider an <strong>example</strong>.</p>

<h3 id="environment">Environment</h3>

<p>For simplicity, suppose there are only two states in a gridworld: one start state and one terminal state. The start state is on the left of the terminal state. Legal actions in non-terminal states (in this case, the start state only) are up, right, down and left and each action costs a reward of -1. After the agent arrives at the terminal state, the episode terminates and no further rewards are incurred. If the agent takes an action that makes itself outside of the gridworld, it is moved back to its last grid cell and receives a reward of -1.</p>

<h3 id="initial-q-table">Initial q-table</h3>

<p>To solve this problem, we store the value of each state-action pair (i.e., store the q-table) and do policy iteration.</p>

<p>Suppose we use <script type="math/tex">\gamma = 1</script> and the agent’s policy is deterministic. First, we randomly initialize the q-table, which is the initial uninformed estimate of the true q-table given the environment and the policy. Then we set the agent’s policy to be greedy with respect to this q-table.</p>

<p>Suppose this q-table looks like this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State</th>
      <th style="text-align: center">Action</th>
      <th style="text-align: center">Q-value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">start state</td>
      <td style="text-align: center">up</td>
      <td style="text-align: center">0.15</td>
    </tr>
    <tr>
      <td style="text-align: center">start state</td>
      <td style="text-align: center">right</td>
      <td style="text-align: center">-0.09</td>
    </tr>
    <tr>
      <td style="text-align: center">start state</td>
      <td style="text-align: center">down</td>
      <td style="text-align: center">0.015</td>
    </tr>
    <tr>
      <td style="text-align: center">start state</td>
      <td style="text-align: center">left</td>
      <td style="text-align: center">-0.2</td>
    </tr>
  </tbody>
</table>

<h3 id="policy-evaluation-update-formula">Policy-evaluation update formula</h3>

<p>First, we do policy evaluation. Note that the agent’s policy is NOT updated at all during this procedure. Let’s derive an update rule for policy evaluation. The relationship between <script type="math/tex">V_{\pi}</script> and <script type="math/tex">Q_\pi</script> is listed below:</p>

<script type="math/tex; mode=display">V_{\pi}(s) = \sum_{a} \pi(a \mid s) Q_{\pi}(s, a)</script>

<script type="math/tex; mode=display">Q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma V_{\pi}(s) \right]</script>

<p>Since we store the q-table, we would like both the LHS and RHS to be expressed in terms of <script type="math/tex">Q_\pi</script>. To do this, we substitue equation 1 into equation 2 and get:</p>

<script type="math/tex; mode=display">Q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') Q_{\pi}(s', a') \right]</script>

<p>which can be easily turned into the following update rule:</p>

<script type="math/tex; mode=display">Q_{\pi}(s, a) \leftarrow \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') Q_{\pi}(s', a') \right]</script>

<p>We say policy evaluation has convergenced when the LHS and the RHS of the update rule above is the same. In other words, the stepsize of each update needs to converge to zero.</p>

<h4 id="no-discounting-and-deterministic-policy">No discounting and deterministic policy</h4>

<p>Since (state, up) has the highest initial value, the policy always instruct the agent to move up when the agent is in the start state. However,  moving up in start state, according to the rules of this environment, return the agent back to the start state and the story repeats for an infinite number of times. Therefore, the true value of (state, up) given the environment and the current policy is negative infinity.</p>

<p>The corresponding update rule looks like this:</p>

<script type="math/tex; mode=display">Q_{\pi}(\text{start}, \uparrow) \leftarrow (-1) + Q_{\pi}(\text{start}, \uparrow)</script>

<p>which clearly drives <script type="math/tex">Q_\pi(\text{start}, \uparrow)</script> to negative infinity if repeated for an infinite number of times. However, in this case, we cannot check for convergence because the stepsize is always one.</p>

<h4 id="discounting-and-deterministic-policy">Discounting and deterministic policy</h4>

<p>Warning: This analysis here is not meant to be exhaustive. I haven’t looked at the formal proof of why policy evaluation requires either discounting or non-deterministic policy. Nevertheless, I hope they give you a good intuition on why things work / not work.</p>

<p>The <strong>true value</strong> of <script type="math/tex">Q_\pi(\text{start}, \uparrow)</script>: <script type="math/tex">(-1) + \gamma(-1) + \gamma^2 (-1) + \cdots = \sum_{k=0}^{\infty} \gamma^k (-1) = \frac{(-1)}{1 - \gamma}</script></p>

<p>The corresponding update rule looks like this:</p>

<script type="math/tex; mode=display">Q_{\pi}(\text{start}, \uparrow) \leftarrow (-1) + \gamma Q_{\pi}(\text{start}, \uparrow)</script>

<p>The first estimate is related to the initial estimate by the expression:</p>

<script type="math/tex; mode=display">Q_{\pi}^1(\text{start}, \uparrow) = (-1) + \gamma Q_{\pi}^0(\text{start}, \uparrow)</script>

<p>The second estimate is related to the initial estimate by the expression:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Q_{\pi}^2(\text{start}, \uparrow) &= (-1) + \gamma Q_{\pi}^1(\text{start}, \uparrow) \\
&= (-1) + \gamma \left[ (-1) + \gamma Q_{\pi}^0(\text{start}, \uparrow) \right] \\
&= (-1) + \gamma (-1) + \gamma^2 Q_{\pi}^0(\text{start}, \uparrow)
\end{align} %]]></script>

<p>In general, the <script type="math/tex">n</script>-th estimate is related to the initial estimate by the expression:</p>

<script type="math/tex; mode=display">Q_{\pi}^n(\text{start}, \uparrow) = \sum_{k=0}^{n-1} \gamma^{k} (-1) + \gamma^n Q_{\pi}^0(\text{start}, \uparrow)</script>

<p>As <script type="math/tex">n</script> reaches infinity:</p>

<script type="math/tex; mode=display">Q_{\pi}^{\infty}(\text{start}, \uparrow)  =\sum_{k=0}^{\infty} \gamma^{k}(-1) = \frac{-1}{1 - \gamma}</script>

<p>We just showed that <strong>the estimated value converge to the true value in the limit</strong>.</p>

<p>Now consider the <strong>upper bound</strong> on the difference between the true value and the <script type="math/tex">n</script>-th estimated value:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\left\vert Q_{\pi}^{\infty}(\text{start}, \uparrow) - Q_{\pi}^n(\text{start}, \uparrow) \right\vert &= \left\vert \sum_{k=0}^{\infty} \gamma^{k}(-1) - \sum_{k=0}^{n-1} \gamma^{k} (-1) - \gamma^n Q_{\pi}^0(\text{start}, \uparrow) \right\vert \\
&= \left\vert \sum_{k=n}^{\infty} \gamma^{k}(-1) - \gamma^n Q_{\pi}^0(\text{start}, \uparrow) \right\vert \\
&\leq \left\vert \sum_{k=n}^{\infty} \gamma^{k}(-1) \right\vert + \left\vert \gamma^n Q_{\pi}^0(\text{start}, \uparrow)  \right\vert
\end{align} %]]></script>

<p>As <script type="math/tex">n</script> increases, this upper bound decreases at a decreasing rate (a bit mouthful but it’s true). This shows that, unlike in the last setting we considered, the stepsize decreases as we approach the goal. This allows us to stop policy iteration when the difference between the RHS and the LHS of the update rule reaches some preset precision tolerance.</p>

<p>Hint for deriving the upper bound: <script type="math/tex">\left\vert a - b \right\vert = \left\vert a + (-b) \right\vert \leq \left\vert a \right\vert +  \left\vert -b \right\vert = \left\vert a \right\vert +  \left\vert b \right\vert</script></p>

<h4 id="no-discounting-and-epsilon-greedy-policy">No discounting and epsilon-greedy policy</h4>

<p>The corresponding update rule looks like this:</p>

<script type="math/tex; mode=display">Q_{\pi}(\text{start}, \uparrow) \leftarrow (-1) + (1 - \epsilon)Q_{\pi}(\text{start}, \uparrow) + \frac{\epsilon}{\mid A \mid}  \left[ Q_{\pi}(\text{start}, \uparrow) +Q_{\pi}(\text{start}, \rightarrow) + Q_{\pi}(\text{start}, \downarrow) + Q_{\pi}(\text{start}, \rightarrow) \right]</script>

<p>where <script type="math/tex">\epsilon</script> is the probability assigned to picking a random action (the greedy action can still be picked by chance).</p>

<p>Note that I won’t go into any derivation here because the update rule is a lot more complicated, but the main ideas are still the same.</p>

<ul>
  <li>First, the true value must be finite.</li>
  <li>Second, the estimated value converges to the true value in the limit.</li>
  <li>Third, the difference between the true value and estimated value decreases at a decreasing rate, so policy evaluation can be terminated early on after the difference between the RHS and the LHS of the update rule is lower than some precision tolerance.</li>
</ul>

<h2 id="me-i-would-like-to-learn-a-deterministic-policy-without-using-discounting">Me: I would like to learn a deterministic policy without using discounting.</h2>

<p>Personally, I find neither of these approaches satisfactory.</p>

<ul>
  <li>Discounting is mathematically convenient, but it seems to a bit artificial. For example, in the gridworld setting, without discounting, the value of state-action pair tells us the exact number of actions to take to get to terminal state, given that the reward of every action is -1; however, if we use discounting, this interpretation is not valid anymore.</li>
  <li>Why learn a epsilon-greedy policy when we want a greedy policy more badly?</li>
</ul>

<p>Here are some of my attempts that use <script type="math/tex">\gamma=1</script> and learn a deterministic policy. Both of them worked in practice.</p>

<h3 id="approach-1-decay-epsilon-of-the-epsilon-greedy-policy">Approach 1: Decay epsilon of the epsilon-greedy policy</h3>

<p>Start with <script type="math/tex">\gamma=1</script> and an epsilon-greedy policy. During each policy improvement step, decay epsilon by multiplying it by a constant like 0.8 or 0.9. Check for convergence to optimal value function and optimal policy using the Bellman’s optimality equation for epsilon-greedy policies. Eventually, epsilon becomes so small so that the epsilon-greedy policy is essentially a deterministic policy.</p>

<h3 id="approach-2-truncate-policy-evaluation">Approach 2: Truncate policy evaluation</h3>

<p>This really came as a surprise. When trying approach 1 for large gridworlds (10 by 10), I find that the first round policy evaluation takes forever, so I allowed a larger precision tolerance. The larger this tolerance was, the fast the algorithm ran. Then I thought, the use of very large policy-evaluation tolerance is essentially to truncated policy evaluation, which was mentioned briefly in the value iteration section of the book.</p>

<p>Out of curiosity, I tried truncated policy evaluation starting from a greedy policy directly, and everything worked like a charm! This approach converged at least 5 times faster than approach 1. My implementation alternatives between truncated policy evaluation (one value update for all state-action pairs) + policy improvement.</p>

<p>This might be why it works so well: truncated policy iteration relies on the fact that policy improvement can avoid a state-action pair as long as the value of that pair is lower than other pairs; it does not need to know that the return for that pair is negative infinity.</p>

<h2 id="gallery">Gallery</h2>

<p>Note the difference between  and <code class="highlighter-rouge">conv_tol</code> in the following examples. <code class="highlighter-rouge">pe_tol</code> is the precision tolerance for policy evaluation, while <code class="highlighter-rouge">conv_tol</code> is the precision tolerance for convergence of policy iteration. <code class="highlighter-rouge">pe_tol</code> is the maximum difference allowed between the LHS and the RHS of the update rule (which is really just the Bellman equation), while <code class="highlighter-rouge">conv_tol</code> is the maximum difference allowed between the LHS and RHS of the Bellman optimality equation. More details are available from the <a href="https://github.com/zhihanyang2022/classic_rl/blob/master/examples/policy_iteration/demo.ipynb">notebook</a> and the <a href="https://zhihanyang2022.github.io/classic_rl/index.html">documentation</a>.</p>

<p><strong>Book’s first approach: discounting</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># very fast
</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">EpsilonGreedyPolicy</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">q_initial_estimate</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">PolicyIteration</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> 
    <span class="n">discount_factor</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">truncate_pe</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">pe_tol</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span>
    <span class="n">conv_tol</span><span class="o">=</span><span class="mf">1e-16</span>
<span class="p">)</span>
</code></pre></div></div>

<p><img src="https://i.loli.net/2020/07/23/OX7LfYAsQFqES9J.png" width="500" /></p>

<p><strong>Book’s first approach: epsilon-greedy policy</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># very, very, very slow
# probably for the same reason mentioned in section "No discounting and deterministic policy"
</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">EpsilonGreedyPolicy</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">q_initial_estimate</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">PolicyIteration</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> 
    <span class="n">discount_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">truncate_pe</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">pe_tol</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span>
    <span class="n">conv_tol</span><span class="o">=</span><span class="mf">1e-16</span>
<span class="p">)</span>
</code></pre></div></div>

<p>It was so slow that I didn’t bother waiting for the results. I did verify that the policy evaluation error was decreasing.</p>

<p><strong>My second approach</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># very fast
</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">EpsilonGreedyPolicy</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">q_initial_estimate</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">algo</span> <span class="o">=</span> <span class="n">PolicyIteration</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> 
    <span class="n">discount_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">truncate_pe</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">pe_tol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">conv_tol</span><span class="o">=</span><span class="mf">1e-16</span>
<span class="p">)</span>
</code></pre></div></div>

<p><img src="https://i.loli.net/2020/07/23/ROTDc34wMe6ZAhG.png" width="500" /></p>

<p>Here’s a more sophicated environment with walls and traps (large negative rewards of -5):</p>

<p><img src="https://i.loli.net/2020/07/23/almtEWnvpL3Frgh.png" width="500" /></p>


		<p><a href="/rl">Back</a> to the list of RL posts.</a>
	</div>
</body>
</html>