<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.0 -->
<title>Notes on Policy Gradient | Zhihan’s Deep RL Notes</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Notes on Policy Gradient" />
<meta name="author" content="Zhihan Yang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Zhihan’s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes." />
<meta property="og:description" content="Zhihan’s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes." />
<link rel="canonical" href="http://localhost:4000/math/2020/06/23/policy-gradient.html" />
<meta property="og:url" content="http://localhost:4000/math/2020/06/23/policy-gradient.html" />
<meta property="og:site_name" content="Zhihan’s Deep RL Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-23T23:02:25-05:00" />
<script type="application/ld+json">
{"description":"Zhihan’s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes.","author":{"@type":"Person","name":"Zhihan Yang"},"@type":"BlogPosting","url":"http://localhost:4000/math/2020/06/23/policy-gradient.html","headline":"Notes on Policy Gradient","dateModified":"2020-06-23T23:02:25-05:00","datePublished":"2020-06-23T23:02:25-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/math/2020/06/23/policy-gradient.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Zhihan's Deep RL Notes" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Zhihan&#39;s Deep RL Notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Notes on Policy Gradient</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-06-23T23:02:25-05:00" itemprop="datePublished">Jun 23, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul id="markdown-toc">
  <li><a href="#vanilla-policy-gradient" id="markdown-toc-vanilla-policy-gradient">Vanilla policy gradient</a></li>
  <li><a href="#comparison-to-supervised-learning" id="markdown-toc-comparison-to-supervised-learning">Comparison to supervised learning</a></li>
  <li><a href="#vanilla-reinforce" id="markdown-toc-vanilla-reinforce">Vanilla REINFORCE</a></li>
  <li><a href="#variants-of-vanilla-policy-gradient" id="markdown-toc-variants-of-vanilla-policy-gradient">Variants of vanilla policy gradient</a>    <ul>
      <li><a href="#reward-to-go" id="markdown-toc-reward-to-go">Reward-to-go</a></li>
      <li><a href="#simple-baseline-normalized-advantages" id="markdown-toc-simple-baseline-normalized-advantages">Simple baseline (normalized advantages)</a>        <ul>
          <li><a href="#intuition-of-subtracting-mean" id="markdown-toc-intuition-of-subtracting-mean">Intuition of subtracting mean</a></li>
          <li><a href="#intuition-of-dividing-by-variance" id="markdown-toc-intuition-of-dividing-by-variance">Intuition of dividing by variance</a></li>
          <li><a href="#analyze-variance" id="markdown-toc-analyze-variance">Analyze variance</a></li>
        </ul>
      </li>
      <li><a href="#optimal-baseline-minimize-variance" id="markdown-toc-optimal-baseline-minimize-variance">Optimal baseline (minimize variance)</a></li>
      <li><a href="#neural-network-baseline" id="markdown-toc-neural-network-baseline">Neural network baseline</a></li>
    </ul>
  </li>
  <li><a href="#experimental-results" id="markdown-toc-experimental-results">Experimental results</a>    <ul>
      <li><a href="#cumulative-reward-over-time" id="markdown-toc-cumulative-reward-over-time">Cumulative reward over time</a></li>
      <li><a href="#trajectory-sampled-under-optimal-policy" id="markdown-toc-trajectory-sampled-under-optimal-policy">Trajectory sampled under optimal policy</a></li>
    </ul>
  </li>
</ul>
<h2 id="vanilla-policy-gradient">Vanilla policy gradient</h2>

<p>The policy gradient, just like any other gradient, point to the direction of fastest increase of some objective funcion.</p>

<p><strong>Goal.</strong> We seek to maximize the following quantity:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&J(\theta)\\ 
=& \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ r(\tau) \right] \\
=& \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=1}^T r(s_t, a_t) \right]
\end{align*} %]]></script>

<p>where:</p>

<ul>
  <li><script type="math/tex">\theta</script> is the parameters of a neural network.</li>
  <li><script type="math/tex">\tau = \left\{s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_{T-1}, a_{T-1}, r_{T-1}, s_T \right\}</script> is a <strong>finite</strong> trajectory of alternating states, actions and rewards.</li>
  <li><script type="math/tex">\pi_{\theta}(\tau) = p(s_1) \prod_{t=1}^{T-1} \pi_\theta(a_t \mid s_t) p(s_{t+1} \mid s_t, a_t)</script> is the probability of <script type="math/tex">\tau</script> under <script type="math/tex">\pi_{\theta}</script>.</li>
  <li><script type="math/tex">r</script> is the reward function (built-in to the environment).</li>
</ul>

<p><strong>Idea.</strong> <script type="math/tex">\theta_{\text{new}} = \theta_{\text{old}} + \alpha \nabla_{\theta} J(\theta)</script>, now we need to consider how to evaluate <script type="math/tex">\nabla_{\theta} J(\theta)</script>.</p>

<p><strong>Derivation.</strong> Here we derive an easy-to-evaluate form of <script type="math/tex">\nabla_{\theta} J(\theta)</script>.</p>

<p>Identity 1: <script type="math/tex">\nabla_v f(v) = f(v) \frac{\nabla_v f(v)}{f(v)}=f(v) \nabla_v \log f(v)</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nabla_{\theta} J(\theta) 
&= \nabla_\theta \left\{ \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ r(\tau) \right] \right\} \\
&= \nabla_\theta \left\{ \int \pi_\theta(\tau) r(\tau) d\tau \right\} \\
&= \int \nabla_\theta \left\{ \pi_\theta(\tau) \right\} r(\tau) d\tau\\
&= \int \pi_\theta(\tau) \nabla_\theta \left\{ \log\pi_\theta(\tau) \right\} r(\tau) d\tau \\
&= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_\theta \left\{ \log \pi_\theta(\tau) \right\} r(\tau) \right] \\
&= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_\theta \left\{ \log p(s_1) + \sum_{t=1}^{T-1} \log(\pi_\theta(a_t \mid s_t)) + \sum_{t=1}^{T-1} \log(p(s_{t+1} \mid s_t, a_t)) \right\} r(\tau) \right] \\
&= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_\theta \left\{\sum_{t=1}^{T-1} \log(\pi_\theta(a_t \mid s_t))\right\} r(\tau) \right] \\
&= 
\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ 
\underbrace{
\left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_t \mid s_t)) \right\} \right) 
}_{\text{gradient in favor of } \tau}
\underbrace{r(\tau)}_{\text{ reward of } \tau} 
\right] \\
\end{align*} %]]></script>

<p>In practice, this expectation can be evaluated by sampling trajectories using <script type="math/tex">\pi_{\theta}</script> (does not need to be optimal).</p>

<p>We can interpret the purpose of this gradient as increasing the probability of high reward trajectories and decreasing the probability of low reward trajectories.</p>

<h2 id="comparison-to-supervised-learning">Comparison to supervised learning</h2>

<p>We seek to maximize the following quantity using the maximum likelihood approach:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&J_{\text{ML}}(\theta) \\
=&\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\log \pi_{\theta} (\tau)\right] \\
=&\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\sum_{t=1}^{T-1} \log \pi_{\theta} (a_t \mid s_t)\right] 
\end{align*} %]]></script>

<p>This is what we did last time in behavior cloning.</p>

<p>Note that the expectation is over trajectories sampled from the training distribution, not the on-policy distribution.</p>

<p>The easy-to-evaluate form of its gradient can be derived as follows. The important thing to note that is we no longer need to differentiate the distribution of <script type="math/tex">\tau</script> w.r.t. <script type="math/tex">\theta</script> anymore.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nabla_{\theta} \left\{ J_{\text{ML}}(\theta) \right\} &=\nabla_{\theta} \left\{ \mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\sum_{t=1}^{T-1} \log p_{\theta} (a_t \mid s_t)\right]  \right\} \\
&= \nabla_{\theta} \left\{ \int p_{\text{train}}(\tau) \left[\sum_{t=1}^{T-1} \log p_{\theta} (a_t \mid s_t)\right] d\tau \right\} \\
&= \int  p_{\text{train}}(\tau) \nabla_{\theta} \left\{ \sum_{t=1}^{T-1} \log p_{\theta} (a_t \mid s_t)\right\} d\tau\\
&= \int  p_{\text{train}}(\tau) \left\{ \sum_{t=1}^{T-1}  \nabla_{\theta}\log p_{\theta} (a_t \mid s_t)\right\} d\tau\\
&= \mathbb{E}_{\tau \sim p_{\text{train}}(\tau)} \left[ 
\underbrace{\sum_{t=1}^{T-1}  \nabla_{\theta}\log p_{\theta} (a_t \mid s_t)}_{\text{gradient in favor of }\tau} 
\right]
\end{align*} %]]></script>

<p>The differences between behavior cloning and vanilla policy gradient are summarized below:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Method</th>
      <th style="text-align: center">Policy gradient</th>
      <th style="text-align: center">Maximum likelihood (behavior cloning)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Function to maximize</td>
      <td style="text-align: center"><script type="math/tex">\mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=1}^{T-1} r(s_t, a_t) \right]</script></td>
      <td style="text-align: center"><script type="math/tex">\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\sum_{t=1}^{T-1} \log \pi_{\theta} (a_t, s_t)\right]</script></td>
    </tr>
    <tr>
      <td style="text-align: center">Gradient</td>
      <td style="text-align: center"><script type="math/tex">\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \underbrace{\left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_t \mid s_t)) \right\} \right) }_{\text{gradient in favor of } \tau}\underbrace{r(\tau)}_{\text{ reward of } \tau} \right]</script></td>
      <td style="text-align: center"><script type="math/tex">\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)} \left[ \sum_{t=1}^{T-1} \nabla_{\theta}\log p_{\theta} (a_t, s_t) \right]</script></td>
    </tr>
    <tr>
      <td style="text-align: center">MC gradient estimate</td>
      <td style="text-align: center"><script type="math/tex">\frac{1}{N} \sum_{n=1}^N \left\{ \underbrace{\left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \right) }_{\text{gradient in favor of } \tau_n}\underbrace{r(\tau_n)}_{\text{ reward of } \tau_n} \right\}</script></td>
      <td style="text-align: center"><script type="math/tex">\frac{1}{N} \sum_{n=1}^N \left\{ \underbrace{\sum_{t=1}^{T-1} \nabla_{\theta}\log p_{\theta} (a_{n, t} \mid s_{n, t})}_{\text{gradient in favor of } \tau_n} \right\}</script></td>
    </tr>
  </tbody>
</table>

<p>Notes:</p>

<ul>
  <li>In English, <script type="math/tex">\nabla_{\theta} J(\theta)</script> weights the gradient in favor of <script type="math/tex">\tau_n</script> by its reward, while <script type="math/tex">\nabla_{\theta}J_{\text{ML}}(\theta)</script> weights all gradients equally.</li>
  <li>The similarity between two gradients will help us compute the policy gradient.</li>
</ul>

<h2 id="vanilla-reinforce">Vanilla REINFORCE</h2>

<ul>
  <li>Initialize <script type="math/tex">\theta</script>.</li>
  <li>Loop:
    <ul>
      <li>Sample set of <script type="math/tex">\tau_n</script> by running <script type="math/tex">\pi_{\theta}(a_t \mid s_t)</script> in some environment.</li>
      <li>Compute <script type="math/tex">\nabla_{\theta}J(\theta)</script> by using its MC gradient estimate rule in the table above.</li>
      <li><script type="math/tex">\theta_{\text{new}} \leftarrow \theta_{\text{old}} + \alpha \nabla_{\theta}J(\theta)</script>. (A more advanced optimizer can be used in practice.)</li>
    </ul>
  </li>
</ul>

<h2 id="variants-of-vanilla-policy-gradient">Variants of vanilla policy gradient</h2>

<p>Reference: answer by Jerry Liu: <a href="https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance">Why does the policy gradient method have a high variance?</a></p>

<h3 id="reward-to-go">Reward-to-go</h3>

<p>Since we will be estimating policy gradients by sampling trajectories, the variance of the resulting gradients can be high. To reduce this variance, we need to eliminate as many random variables as possible from the MC gradient estimate formula. To do so, we first re-write the MC gradient estimate formula as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\nabla_{\theta}J(\theta) \\
\approx& \frac{1}{N} \sum_{n=1}^N \left\{ \left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \right) \left( \sum_{t=1}^{T-1} r(s_{n, t}, a_{n, t}) \right)\right\} \\
=& \frac{1}{N} \sum_{n=1}^N \left\{ \left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \left( \sum_{t'=1}^{T-1} r(s_{n, t'}, a_{n, t'}) \right)\right) \right\} \\
=& \frac{1}{N} \sum_{n=1}^N \left\{ \left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \left( \sum_{t'=t}^{T-1} r(s_{n, t'}, a_{n, t'}) \right)\right) \right\} \\
\end{align*} %]]></script>

<p>where we exploited causality (future actions do not impact past rewards) in the last step to remove all <script type="math/tex">r(s_{n, t’}, a_{n, t’})</script> where <script type="math/tex">% <![CDATA[
t’ < t %]]></script>.</p>

<p>To see why each reward is a random variable, consider the reward of the <script type="math/tex">k</script>-th state on the <script type="math/tex">i</script>-th sampled trajectory. Obviously, this reward, <script type="math/tex">r(s_{i, k}, a_{i, k})</script> can be a random variable depending on the outcomes of following random processes:</p>

<ul>
  <li>Randomness in reward (aka. the reward function itself maybe stochastic).</li>
  <li>Randomness in what <script type="math/tex">s_{i, k}</script> is.</li>
  <li>Randomness in what <script type="math/tex">a_{i, k}</script> is given <script type="math/tex">s_{i, k}</script>.</li>
</ul>

<h3 id="simple-baseline-normalized-advantages">Simple baseline (normalized advantages)</h3>

<p>The reason why this trick is called “normalized advantages” is because the scalar multiplier of each gradient term can be seen as its <em>advantage</em>: if a gradient term has a larger multiplier relative to other gradient terms, a gradient-descent step benefits its objective function more than that of other gradient terms. What <em>normalized</em> mean below is described below.</p>

<p>Vanilla policy gradient + simple baseline:</p>

<script type="math/tex; mode=display">\begin{align*}
\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{n=1}^N \left\{ \nabla_\theta \left\{ \log\pi_\theta(\tau_{n}) \right\} \left( \frac{r(\tau_n) - \mu}{\sigma}\right) \right\}
\end{align*}</script>

<p>where <script type="math/tex">\mu</script> and <script type="math/tex">\sigma</script> are the mean and standard deviation of <script type="math/tex">\left\{ r(\tau_n) \right\}</script>. Doing so does not change the expected value of the gradient because:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\mathbb{E}\left[ \nabla_{\theta} \left\{ \log \pi_{\theta}(\tau) b \right\} \right] \\
=& \int \pi_{\theta}(\tau) \nabla_{\theta} \left\{ \log \pi_{\theta}(\tau) b \right\} d\tau \\
=& b \int \pi_{\theta}(\tau) \nabla_{\theta} \left\{ \log \pi_{\theta}(\tau) \right\} d\tau \\
=& b \int \nabla_{\theta} \pi_{\theta}(\tau) d\tau \\
=& b \nabla_{\theta} \int \pi_{\theta}(\tau) d\tau \\
=& b \nabla_{\theta} 1 = 0
\end{align*} %]]></script>

<p>In practice, it is often more convenient to compute the mean and standard deviation of <script type="math/tex">\left\{ r(a_{n, t} \mid s_{n, t}) \right\}</script> and normalize these. For example, in reward-to-go, there is no such thing as “the reward of an entire trajectory” because the reward assigned to each state along a trajectory is different.</p>

<p>In terms of performance, using this trick can drastically improve the stability of vanilla REINFORCE.</p>

<h4 id="intuition-of-subtracting-mean">Intuition of subtracting mean</h4>

<p>Suppose we sampled three trajectory and all of them yielded large positive rewards: 999, 1000, 1111. Then by the vanilla policy gradient, all three trajectories are made more likely. However, to further improve, we only care about trajectories that yield <em>higher-than-average</em> rewards. Therefore, we subtract <script type="math/tex">\mu</script> from each reward. In this way, the updated rewards are: -1, 0, 1. In other words, the third trajectories is made more likely and the first trajectory is made less likely.</p>

<h4 id="intuition-of-dividing-by-variance">Intuition of dividing by variance</h4>

<p>Let’s consider a hypothetical environment. This environment is initialized such that it satisfies some condition <script type="math/tex">A</script>. At each time-step, the agent interacts with the environment, receives an reward of 1 and observes the updated environment. This continues on until the updated environment no longer satisfies condition <script type="math/tex">A</script>. We force the environment to terminate after 200 time-steps.</p>

<p>Suppose we have a competent RL agent. Then its expected reward of each trajectory should look like this:</p>

<ul>
  <li>Early phase of learning: very small</li>
  <li>Final phase of learning: close to 200 (200 time-steps in total; for each time-step, only consider the total reward in the future, according to reward-to-go)</li>
</ul>

<p>To see why this can be a problem, consider the vanilla policy gradient:</p>

<script type="math/tex; mode=display">\begin{align*}
\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{n=1}^N \left\{ \underbrace{\nabla_\theta \left\{ \log(\pi_\theta(\tau_{n}) \right\}}_{\text{gradient term}} \underbrace{r(\tau_n)}_{\text{scalar multiplier}} \right\}
\end{align*}</script>

<p>Each gradient term gets multiplied by a small scalar in the early phase of learning but gets multiplied by a much large scalar later on. This might be very undesirable for gradient descent, and this problem will get worse if we put a large cap on episode length. Therefore, normalization of rewards can help gradient descent perform better, provided that a proper optimizer and a proper learning rate have been chosen.</p>

<h4 id="analyze-variance">Analyze variance</h4>

<p>For some trajectory <script type="math/tex">\tau_n</script>, its contribution to the vanilla policy gradient <script type="math/tex">\nabla_{\theta}J(\theta)</script> is <script type="math/tex">\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\} r(\tau_n)</script>. Since <script type="math/tex">\tau_n</script> is the outcome of a random process and <script type="math/tex">\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\} r(\tau_n)</script> depends on this outcome, <script type="math/tex">\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\} r(\tau_n)</script> is the product of two random variables. The variance of two independent variables <script type="math/tex">X</script> and <script type="math/tex">Y</script> is <script type="math/tex">\mu_X^2 \sigma_Y^2 + \mu_Y^2 \sigma_X^2 + \sigma_X^2 \sigma_Y^2</script>. Assuming that <script type="math/tex">\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}</script> and <script type="math/tex">r(\tau_n)</script> are independent, the variance of their product can be written as</p>

<script type="math/tex; mode=display">\begin{align*}
E[\nabla_{\theta} \left\{ \log\pi_\theta(\tau) \right\}]^2 \text{Var}[r(\tau)] + E[r(\tau)]^2 \text{Var}[\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}] + \text{Var}[\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}]\text{Var}[r(\tau)].
\end{align*}</script>

<p>If <script type="math/tex">r(\tau_n)</script> is normalized (has a mean of zero and a variance of one), the variance of their product is now</p>

<script type="math/tex; mode=display">\begin{align*}
E[\nabla_{\theta} \left\{ \log\pi_\theta(\tau) \right\}]^2 + 2 \text{Var}[\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}],
\end{align*}</script>

<p>which is reduced. A more involved analysis can be used to select <script type="math/tex">b</script> that not only reduces but also minimizes the variance.</p>

<h3 id="optimal-baseline-minimize-variance">Optimal baseline (minimize variance)</h3>

<p>TODO</p>

<h3 id="neural-network-baseline">Neural network baseline</h3>

<p>TODO</p>

<h2 id="experimental-results">Experimental results</h2>

<p>Jupyter notebook is available <a href="https://github.com/zhihanyang2022/deeprl_notes/blob/master/02_policy_gradient/cartpole_v0_policy_gradient_solver.ipynb">here</a>.</p>

<h3 id="cumulative-reward-over-time">Cumulative reward over time</h3>

<p><img src="https://raw.githubusercontent.com/zhihanyang2022/deeprl_notes_site/master/_posts/policy_gradient/learning_curve.png" /></p>

<p>Legend:</p>

<ul>
  <li>cartpole_v0: <a href="https://gym.openai.com/envs/CartPole-v0/">CartPole-v0 (link to OpenAI’s gym page)</a></li>
  <li>pg: solved using policy gradient</li>
  <li>50: trained for 50 iterations</li>
  <li>100: for each iteration, 100 trajectories are sampled</li>
  <li>rtg: reward-to-go</li>
  <li>na: simple baseline (normalized advantages)</li>
  <li>solid lines: median performance</li>
  <li>shaped regions: 0.25 to 0.75 quantile performance</li>
</ul>

<p>Observations:</p>

<ul>
  <li>na seems to determine the rate of convergence initially (na <script type="math/tex">\to</script> high rate of convergence initially).</li>
  <li>rtg seems to determine whether a variant actually converges (rtg <script type="math/tex">\to</script> convergence eventually reached).</li>
</ul>

<h3 id="trajectory-sampled-under-optimal-policy">Trajectory sampled under optimal policy</h3>

<p><img src="https://raw.githubusercontent.com/zhihanyang2022/deeprl_notes_site/master/_posts/policy_gradient/trajectory_under_learned_policy.png" /></p>


  </div><a class="u-url" href="/math/2020/06/23/policy-gradient.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Zhihan&#39;s Deep RL Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Zhihan Yang</li><li><a class="u-email" href="mailto:yangz2@carleton.edu">yangz2@carleton.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Zhihan&#39;s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
