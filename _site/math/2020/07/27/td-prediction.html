<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.0 -->
<title>TD Prediction | Zhihan’s Deep RL Notes</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="TD Prediction" />
<meta name="author" content="Zhihan Yang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Zhihan’s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes." />
<meta property="og:description" content="Zhihan’s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes." />
<link rel="canonical" href="http://localhost:4000/math/2020/07/27/td-prediction.html" />
<meta property="og:url" content="http://localhost:4000/math/2020/07/27/td-prediction.html" />
<meta property="og:site_name" content="Zhihan’s Deep RL Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-27T10:00:00-05:00" />
<script type="application/ld+json">
{"description":"Zhihan’s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes.","author":{"@type":"Person","name":"Zhihan Yang"},"@type":"BlogPosting","url":"http://localhost:4000/math/2020/07/27/td-prediction.html","headline":"TD Prediction","dateModified":"2020-07-27T10:00:00-05:00","datePublished":"2020-07-27T10:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/math/2020/07/27/td-prediction.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Zhihan's Deep RL Notes" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Zhihan&#39;s Deep RL Notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">TD Prediction</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-07-27T10:00:00-05:00" itemprop="datePublished">Jul 27, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul id="markdown-toc">
  <li><a href="#on-policy-prediction" id="markdown-toc-on-policy-prediction">On-policy prediction</a></li>
  <li><a href="#off-policy-prediction" id="markdown-toc-off-policy-prediction">Off-policy prediction</a></li>
</ul>
<h2 id="on-policy-prediction">On-policy prediction</h2>

<p>The Bellman equation for <script type="math/tex">v_{\pi}</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
v_{\pi}(s) &= \sum_{a} \pi(a \mid s) q_{\pi}(s, a) \\
&= \sum_{a} \pi(a \mid s)\sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi}(s') \right]
\end{align} %]]></script>

<p>In TD, we make both the outer and inner expectation implicit through sampling. We simply estimate the value of <script type="math/tex">s</script> as the average of all boostrapped returns following visits to <script type="math/tex">s</script>. Note that each boostrapped return is a sample of <script type="math/tex">R + v_{\pi}(S')</script>, where <script type="math/tex">R</script> is the reward of taking <script type="math/tex">\pi(s)</script> in <script type="math/tex">s</script> and <script type="math/tex">S’</script> is the next state of taking <script type="math/tex">\pi(s)</script> in <script type="math/tex">s</script>. The online update rule is shown below. A relevant derivation can be found in the TD Control post.</p>

<script type="math/tex; mode=display">V(S) \leftarrow V(S) + \frac{1}{n} \left[ R + \gamma V(S') - V(S) \right]</script>

<p>Since value estimates are more accurate over time, we would like to give recent updates more weight; the new online update rule is:</p>

<script type="math/tex; mode=display">V(S) \leftarrow V(S) + \alpha \left[ R + \gamma V(S') - V(S) \right] = (1 - \alpha) V(S) + \alpha (R + \gamma V(S'))</script>

<p>where <script type="math/tex">\alpha \in (0, 1]</script> is a constant stepsize, unlike <script type="math/tex">\frac{1}{n}</script>.</p>

<p>Finally, we arrive at the algorithm below:</p>

<p><img src="https://i.loli.net/2020/07/28/rbXAlVI4tye1JKg.png" alt="image-20200727113505484" /></p>

<h2 id="off-policy-prediction">Off-policy prediction</h2>

<p>The idea behind off-policy methods is actually strikingly simple:</p>

<ul>
  <li>A behavior policy is used to meet all the states.</li>
  <li>The update rule estimates the value function with respect to the target policy.</li>
</ul>

<p>Following this idea, we can easily design an off-policy version of TD prediction:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: an arbitrary target policy pi
Algorithm parameter: alpha in (0, 1]
Initialize V(s), for all s in S+, with V(terminal) = 0

Loop for each episode:
    b &lt;- any policy with coverage of pi
    Loop for each step of episode:
        
        # the behavior policy is justed for visiting all states
        # we don't care about the returns
        
        A &lt;- action given by b for S
        Take action A, observe S'
        
        # the target policy is used for actual value updates
        
        A* &lt;- action given by pi for S
        Take action A*, observe R*, S*'
        
        V(S) &lt;- V(S) + alpha[R* + gamma * V(S*') - V(S*)]
        
        S &lt;- S'
        
    until S is terminal.
</code></pre></div></div>

<p>But this algorithm assumes that we can take an action in some state, and then immediately return to that state and take a different action. In general, this is not possible. To get rid of this assumption, we 1) use the reward and next state under the behavior policy in the value update formula but 2) correct that formula with a weighted importance sampling ratio to account for the fact that what we really want are samples under the target policy.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: an arbitrary target policy pi
Algorithm parameter: alpha in (0, 1]
Initialize V(s), for all s in S+, with V(terminal) = 0
Initialize, for all s in S:
    Q(S) in R
    C(S) &lt;- 0

Loop for each episode:
    b &lt;- ay policy with coverage of pi
    Loop for each step of episode:
        
        A &lt;- action given by b for S
        Take action A, observe R, S'
        
        W = pi(A|S) / b(A|S)  # importance sampling ratio
        C(S) &lt;- C(S) + W
        V(S) &lt;- V(S) + (W / C(S))[R + gamma * V(S') - V(S)]
        
        S &lt;- S'
        
    until S is terminal.
</code></pre></div></div>

<p>The relevant derivation of these update rules can be found in section 5.6 of Sutton &amp; Barto (2018), although there were derived in terms of Monte Carlo methods. Unfortunately, I don’t think these update rules take into account of the fact that recent samples are more “correct” than old samples.</p>


  </div><a class="u-url" href="/math/2020/07/27/td-prediction.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Zhihan&#39;s Deep RL Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Zhihan Yang</li><li><a class="u-email" href="mailto:yangz2@carleton.edu">yangz2@carleton.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Zhihan&#39;s notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
