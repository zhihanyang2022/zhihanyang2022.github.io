<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-24T14:45:49-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Zhihan’s Deep RL Notes</title><subtitle>Zhihan's notes on deep reinforcement learning. For Pytorch implementations, check out https://github.com/zhihanyang2022/deeprl_notes.</subtitle><author><name>Zhihan Yang</name></author><entry><title type="html">Dyna-Q</title><link href="http://localhost:4000/rl/dynaq" rel="alternate" type="text/html" title="Dyna-Q" /><published>2020-08-11T15:30:00-05:00</published><updated>2020-08-11T15:30:00-05:00</updated><id>http://localhost:4000/rl/dynaq</id><content type="html" xml:base="http://localhost:4000/rl/dynaq">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;Time took to write: 8 days.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-planning&quot; id=&quot;markdown-toc-what-is-planning&quot;&gt;What is planning?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-tabular-one-step-dyna-q-algorithm&quot; id=&quot;markdown-toc-the-tabular-one-step-dyna-q-algorithm&quot;&gt;The tabular one-step Dyna-Q algorithm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-effect-of-planning-on-policy-learning&quot; id=&quot;markdown-toc-the-effect-of-planning-on-policy-learning&quot;&gt;The effect of planning on policy learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-effect-of-different-planning-ratios-on-policy-learning&quot; id=&quot;markdown-toc-the-effect-of-different-planning-ratios-on-policy-learning&quot;&gt;The effect of different planning ratios on policy learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#problems-with-dyna-q&quot; id=&quot;markdown-toc-problems-with-dyna-q&quot;&gt;Problems with Dyna-Q&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#efficiency&quot; id=&quot;markdown-toc-efficiency&quot;&gt;Efficiency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#when-the-model-is-wrong-dyna-q&quot; id=&quot;markdown-toc-when-the-model-is-wrong-dyna-q&quot;&gt;When the model is wrong: Dyna-Q+&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we learn the basic definition of planning in the context of reinforcement learning, and implement a simple algorithm called Dyna that adds planning to the one-step Q-learning algorithm.&lt;/p&gt;

&lt;h2 id=&quot;what-is-planning&quot;&gt;What is planning?&lt;/h2&gt;

&lt;p&gt;In RL, &lt;strong&gt;planning&lt;/strong&gt; (formally known as &lt;em&gt;state-space plannning&lt;/em&gt;) is the evaluation the value function under a policy using simulated experiences generated by the model of an environment (explained shortly). This sounds a lot like &lt;strong&gt;learning&lt;/strong&gt;, the evaluation of the value function under a policy using real experiences. Therefore, they serve the same purpose: in both cases, making the policy greedy or epsilon-greedy with respect to a more accurate value function improves that policy.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;model&lt;/strong&gt; of the environment is anything that an agent can use to predict how the environment will respond to its  actions (i.e., simulate or generate experiences). There are two types of models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distribution models: given a state-action pair, produce all possible next states and their probabilities&lt;/li&gt;
  &lt;li&gt;Sample models: given a state-action pair, produce one possible next state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Distribution models are stronger than sample models because they can do everything a sample model can; however, they are generally harder to obtain for many problems.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Sidenote&lt;/b&gt; Distribution models might be confusing because a distribution over next states is very different from our concept of &lt;i&gt;experience&lt;/i&gt;. A way to understand such models is to view experiences in terms of why we need them in RL: value backup. The value of a state is the expected return following it, and viewing experiences (and thus the corresponding returns) as a distribution help us calculate this expectation. A sample model calculates this same expectation implicitly by sampling individual instances of experience, which make itself more intuitive.&lt;/p&gt;

&lt;p&gt;Here’s a flowchart of how planning works:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\text{model (input)} \to \text{simulated experiences} \to \text{values} \to \text{improved policy (output)}
\end{align*}&lt;/script&gt;

&lt;p&gt;Here possible relationships between experience, model, values, and policy are summarized below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/08/12/wUje1WShKVT8csl.png&quot; alt=&quot;image-20200811155226803&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the diagram above, we that there are at least two roles for real experience:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improve the value function and thus policy using the learning methods discussed in previous chapters (via direct RL)&lt;/li&gt;
  &lt;li&gt;Improve the model (via model learning)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that, in both ways (one direct and one indirect), the value functions and policies are improved. Therefore, these two approaches are also called &lt;strong&gt;direct RL&lt;/strong&gt; and &lt;strong&gt;indirect RL&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;At this point, we don’t attempt to say whether learning or planning is better - they have deep connections. For example, my other post on TD control emphasizes the link between TD control (a model-free method) and value iteration (a model-based method).&lt;/p&gt;

&lt;h2 id=&quot;the-tabular-one-step-dyna-q-algorithm&quot;&gt;The tabular one-step Dyna-Q algorithm&lt;/h2&gt;

&lt;p&gt;For illustration purposes, the following version of the algorithm assumes that the environment is deterministic in terms of next states and rewards. If the code between &lt;code class=&quot;highlighter-rouge&quot;&gt;planning: start&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;planning: end&lt;/code&gt; is removed (or if &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; is set to zero), then we would have the Q-learning algorithm.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Initialize Q(s, a) for all (s, a) pairs with Q(terminal, .) = 0.
Initialize Model to be an empty dictionary.
Initialize VisitedStates to be an empty set.
Initialize ActionsInState to be an empty dictionary.

Set max_episodes.
Set alpha - the learning rate.
Set gamma - the discount factor.
Set n - the planning-learning ratio.

for episode in range(max_episodes):

    Initialize s to be the starting state.
    
    Loop:
        
        # ===== learning: start =====
        
        Choose a using s from the epsilon-greedy (behavior) policy derived from Q.
        Take action a, observe r and s'.
        
        # preparation for planning
        
        VisitedStates.add(s)
        if s in ActionsInState.keys():
            ActionsInState[s].append(s)
        else:
            ActionsInState[s] = [a]
        Model[(s, a)] = (r, s')  
            
        Bellman sample = r + gamma * max_a Q(s', a)
        
        Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample  # update rule

        s = s'
        
        # ===== learning: end =====
        
        # ===== planning: start =====
        
        for _ in range(n):
            
            s = randomly chosen state from VisitedStates
            a = random action chosen from ActionsInState[s]
            r, s' = Model[(s, a)]
            
            Bellman sample = r + gamma * max_a Q(s', a)
            
            Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample  # update rule
            
        # ===== planning: end =====
        			
    until s is terminal.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-effect-of-planning-on-policy-learning&quot;&gt;The effect of planning on policy learning&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Setup&lt;/b&gt; The environment is a 10-by-10 gridworld with a starting state (S) at the top-left corner and a terminal state (T) at the bottom-right corner. All transitions yield zero reward except the ones into the terminal state, which yield a reward of 1. Here, we consider a planning ratio of 20 (20 planning steps per learning step). The learning rate is set to 0.1. The discount factor is set to 0.99 to encourage the agent to find the shortest path(s) to the reward.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Definitions&lt;/b&gt; Before we start the discussion, let’s define some terms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A learning step: an executation of the update rule in learning section of the pseudo-code above&lt;/li&gt;
  &lt;li&gt;A planning step: an executation of the update rule in planning section of the pseudo-code above&lt;/li&gt;
  &lt;li&gt;A effective learning step: a learning step that
    &lt;ul&gt;
      &lt;li&gt;modify the value of some state-action pair before value-function convergence, or&lt;/li&gt;
      &lt;li&gt;does not change the value of any state-action pair after value-function convergence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A effective planning step: a learning step that
    &lt;ul&gt;
      &lt;li&gt;modify the value of some state-action pair before value-function convergence, or&lt;/li&gt;
      &lt;li&gt;does not change the value of any state-action pair after value-function convergence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;b&gt;Observations of the first two episodes&lt;/b&gt; During the first episode (before the terminal state is reached), the values of all state-action pairs are zero. This is not surprising because we initialized them to be. If we take a peek at the value function immediately before the first episode finishes (before the learning step happened for the last state-action pair), we would see that the values of all state-action pairs are still zero. Up to this point, no effective learning or planning steps occurred because:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The values of all state-action pairs are zero.&lt;/li&gt;
  &lt;li&gt;The values of all next state-action pairs are zero (a result of the first problem).&lt;/li&gt;
  &lt;li&gt;All transitions encountered yield zero reward.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since the TD error is defined as &lt;script type=&quot;math/tex&quot;&gt;r + \gamma \max_a Q(s', a) - Q(s, a)&lt;/script&gt; and all three terms above are zero, all TD errors were zero.&lt;/p&gt;

&lt;p&gt;After the final timestep of the first episode, the value of one state-action pair (the one encountered on the final timestep) is non-zero due to an effective learning step. If we happen to pick that state-action or any state-action pair that would lead us to that state-action pair, effective planning steps would happen. This is, of course, quite unlikely because there are 396 state-action pairs (99 non-terminal states times 4 actions per state) but planning is done for only 20 randomly-sampled state-action pairs. Indeed, this did not happen.&lt;/p&gt;

&lt;p&gt;Because zero effective planning steps occurred in the first episode, the greedy action of only one state is defined (Fig. 2), just like in one-step tabular Q-learning; the action values are all zeros for all other states.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Throughout the second episode, there are at most two effective learning steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If the last state-action of the second epsiode is the same as the last state-action of the first episode, then two learning steps occur, one for the last state-action pair, the other for the second last state-action pair.&lt;/li&gt;
  &lt;li&gt;If the last state-action of the second epsiode is different from the last state-action of the first episode, then one learning step occurs for the last state-action pair.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, if no planning steps occurred, the greedy action would be defined for at most two states. However, if we take a look at Fig. 3, we see that the greedy action is defined for a large proportion of states, and you can following the greedy actions from any of these states all the way to the terminal state. Although many of these greedy actions are not optimal, this is still quite amazing!&lt;/p&gt;

&lt;div class=&quot;two-images-container&quot;&gt;
  &lt;div class=&quot;image-container&quot;&gt;
    &lt;img src=&quot;/images/2020-08-11-dynaq/dynaq_n20_after_first_episode_policy.png&quot; class=&quot;image&quot; /&gt;
    &lt;figcaption class=&quot;caption&quot;&gt;
        &lt;b&gt;Figure 2.&lt;/b&gt; Policy learned after first episode for a planning ratio of 20.
    &lt;/figcaption&gt;
  &lt;/div&gt;
  &lt;div class=&quot;image-container&quot;&gt;
    &lt;img src=&quot;/images/2020-08-11-dynaq/dynaq_n20_after_second_episode_policy.png&quot; class=&quot;image&quot; /&gt;
    &lt;figcaption class=&quot;caption&quot;&gt;
        &lt;b&gt;Figure 3.&lt;/b&gt; Policy learned after second episode for a planning ratio of 20.
    &lt;/figcaption&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;b&gt;Analysis of the first two episodes&lt;/b&gt; To fully understand how planning achieved this, consider the total of number of value updates that happened during the second episode. Under a fixed random seed, the length of the second trajectory is 86, which means 86 learning steps and 1720 planning steps (86 learning steps times a planning ratio of 20). Initially, most planning steps were not effective (see the initial flatness of Fig. 4). Nevertheless, there were a few effective planning steps, the ones for state-action pairs around the terminal state that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have non-zero values&lt;/li&gt;
  &lt;li&gt;Lead into state-action pairs with non-zero values&lt;/li&gt;
  &lt;li&gt;Lead into the terminal state (&lt;script type=&quot;math/tex&quot;&gt;r=1&lt;/script&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;because, again, the TD error is defined as &lt;script type=&quot;math/tex&quot;&gt;r + \gamma * \max_a Q(s',a) - Q(s, a)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;As time goes (allowed by a LARGE number of planning steps), the frontier of state-action pairs with non-zero values propagate backwards towards the starting state; as more state-action pairs have non-zero values, the rate of effective planning steps increase (see the increasing slope of Fig. 4). A milder version of the trend applies for the policy as not all changes in the value function is reflected by the policy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-11-dynaq/dynaq_n20_efficiency.png&quot; width=&quot;350&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;width: 500px; margin: auto&quot; class=&quot;caption&quot;&gt;
  &lt;b&gt;Figure 4.&lt;/b&gt; The cumulative number of action updates against the number of planning steps during the second episode for a planning ratio of 20.
&lt;/figcaption&gt;

&lt;h2 id=&quot;the-effect-of-different-planning-ratios-on-policy-learning&quot;&gt;The effect of different planning ratios on policy learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-11-dynaq/dynaq_online_performance.png&quot; width=&quot;350&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;width: 500px; margin: auto&quot; class=&quot;caption&quot;&gt;
  &lt;b&gt;Figure 5.&lt;/b&gt; Episode length (the shorter the better) vs. number of episodes for a planning ratio of 0 (Q-learning), 1 and 20.
&lt;/figcaption&gt;

&lt;h2 id=&quot;problems-with-dyna-q&quot;&gt;Problems with Dyna-Q&lt;/h2&gt;

&lt;h3 id=&quot;efficiency&quot;&gt;Efficiency&lt;/h3&gt;

&lt;p&gt;As shown in Figure 4, the rate of effective value updates is small in the beginning because the values of most state-action pairs are zero and all transitions between most states yield zero reward. In this case, the TD error calculated would be zero for state-action pairs whose value is zero and whose next state-action pairs have zero value. Yet, at the beginning, a uniform selection plan gives a lot of probability to such state-action pairs. Therefore, a lot of useless updates are performed and negligible change in value function or policy is observed at the beginning.&lt;/p&gt;

&lt;p&gt;Can we do better than this?&lt;/p&gt;

&lt;p&gt;Initially, only state-action pairs (let’s call this group of pairs A) leading into the goal state gets updated after learning from the first episode. Planning should focus on state-action pairs leading into group A or terminal state-action pairs because these pairs have non-zero value or non-zero reward respectively, and then work backward to the starting state(s).  In general, however, transitions to goals state don’t necessarily give +1 reward and previous transitions can also yield arbitrary reward. Therefore, we want to work back not just from goal states but from any state whose value has changed significantly.&lt;/p&gt;

&lt;p&gt;This idea, which is called &lt;b&gt;backward focusing&lt;/b&gt;, is the motivation for the Prioritized Sweeping algorithm, which we shall see in another post. In constrast, &lt;b&gt;forward focusing&lt;/b&gt; methods prioritizes updates based on how likely a state-action pair is to appear on actual trajectories and are therefore &lt;i&gt;on-policy&lt;/i&gt;. Both methods intend to improve the efficiency of the value-function computation (finding a better solution in shorter time) by distributing or prioritizing updates.&lt;/p&gt;

&lt;h3 id=&quot;when-the-model-is-wrong-dyna-q&quot;&gt;When the model is wrong: Dyna-Q+&lt;/h3&gt;

&lt;p&gt;So far into the book, we have only considered how to find the optimal policy for a fixed environment. What if the environment changes during the course of training? Would this impact the algorithms’ performance on finding the optimal policy?&lt;/p&gt;

&lt;div class=&quot;two-images-container&quot;&gt;
  &lt;div class=&quot;image-container&quot;&gt;
    &lt;img src=&quot;/images/2020-08-11-dynaq/shortcut_maze_1.png&quot; class=&quot;image&quot; /&gt;
    &lt;br /&gt;
    &lt;figcaption class=&quot;caption&quot;&gt;
        &lt;b&gt;Figure 6.&lt;/b&gt; Initial environment.
    &lt;/figcaption&gt;
  &lt;/div&gt;
  &lt;div class=&quot;image-container&quot;&gt;
    &lt;img src=&quot;/images/2020-08-11-dynaq/shortcut_maze_2.png&quot; class=&quot;image&quot; /&gt;
    &lt;br /&gt;
    &lt;figcaption class=&quot;caption&quot;&gt;
        &lt;b&gt;Figure 7.&lt;/b&gt; New environment.
    &lt;/figcaption&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Example&lt;/b&gt; To make things concrete, consider the environment in Figure 6. Transitions into the terminal state yield a reward of 1 and all other transitions yield a reward of zero. A discount factor of 0.95 is used for shortest-path finding.  Initially, to get from the starting state to the terminal state, the agent needed to go around the walls from the left. After running Dyna-Q for 100 episodes, we suddenly opened up a shorter path (Figure 7) than the shortest path previously available.&lt;/p&gt;

&lt;p&gt;Dyna-Q never discovered this shorter path, while Dyna-Q+ (a modification of Dyna-Q) noticed and continued to exploit it starting from about 50 episodes after the swap (Figure 8). The corresponding policies learned are shown in Figure 9 and 10.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-08-11-dynaq/dynaq_vs_dynaqplus_trajectory_length.png&quot; width=&quot;350&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;width: 500px; margin: auto&quot; class=&quot;caption&quot;&gt;
  &lt;b&gt;Figure 8.&lt;/b&gt; Trajectory lengths over episodes for Dyna-Q and Dyna-Q+. 
&lt;/figcaption&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;two-images-container&quot;&gt;
  &lt;div class=&quot;image-container&quot;&gt;
    &lt;img src=&quot;/images/2020-08-11-dynaq/dynaq_shortcut_maze_2_policy.png&quot; class=&quot;image&quot; /&gt;
    &lt;br /&gt;
    &lt;figcaption class=&quot;caption&quot;&gt;
        &lt;b&gt;Figure 9.&lt;/b&gt; Policy learned by Dyna-Q.
    &lt;/figcaption&gt;
  &lt;/div&gt;
  &lt;div class=&quot;image-container&quot;&gt;
    &lt;img src=&quot;/images/2020-08-11-dynaq/dynaqplus_shortcut_maze_2_policy.png&quot; class=&quot;image&quot; /&gt;
    &lt;br /&gt;
    &lt;figcaption class=&quot;caption&quot;&gt;
        &lt;b&gt;Figure 10.&lt;/b&gt; Policy learned by Dyna-Q+.
    &lt;/figcaption&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Why does Dyna-Q fail?&lt;/b&gt; We will discuss what Dyna-Q+ is in a moment. At this point, simply note that Dyna-Q+ encourages exploration more than Dyna-Q. While all the exploration of Dyna-Q comes from the epsilon-greedy policy, Dyna-Q+ uses an additional heuristic to help the agent explore longer trajectories (consist of multiple sub-optimal actions). As an simple example, under a epsilon-greedy policy with &lt;script type=&quot;math/tex&quot;&gt;\epsilon=0.1&lt;/script&gt;, the probability of taking 3 independent suboptimal actions is almost negligible &lt;script type=&quot;math/tex&quot;&gt;(0.1/3)^3 = 0.0000370 \text{ (3 s.f.)}&lt;/script&gt;, assuming that there is one greedy action and three non-greedy actions in each state (common to gridworld). Because of its limited exploration power, the state-action pair ((3, 8), up) is never taken by Dyna-Q and therefore is never added to its model. As a result, ((3, 8), up) appear in neither real nor simulated experiences and it never inherits the high value of the state-action pair ((2, 8), up) right above it. Because ((3, 8), up) is at the frontier of high values (e.g., the value of ((2, 8), up)) and we are using a one-step update rule, when its value is not updated, the values of state-action pairs below it are also not updated.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Problems with Dyna-Q+&lt;/b&gt; However, any exploration comes at the cost of exploitation. As shown in Figure 8, despite having a better max performance, the episode length of Dyna-Q+ can occasionally be very long (represented by the tall spikes) whenever it explores. As a sidenote, it seems to explore periodically.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Does the model cause Dyna-Q’s problem?&lt;/b&gt; I don’t think so. If we use SARSA or Q-learning, we will likely have the exact same problem as Dyna-Q. The problem is caused by the fact that epsilon-greedy policy is not good for exploration after some policy has already been learned. If the agent goes into a region currently deemed to have low values, then all actions will ask the agent to move backwards, and a small epsilon can’t help much.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Can model be used to ameliorate the problem (Dyna-Q+)?&lt;/b&gt; Yes. The model represents our belief of how the environment works. A simple heuristic to solve the problem is to, in each planning step, use &lt;script type=&quot;math/tex&quot;&gt;r+k \sqrt{\tau}&lt;/script&gt; instead of &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; is the number of timesteps (not episodes!) after the state-action pair was last taken and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; is a hyperparameter. This encodes our belief that the environment changes over time and we optimistically expect unseen state-actions to increase their value over time, although this might not be true for every environment out there. The algorithm is shown below.&lt;/p&gt;

&lt;p&gt;All modifications (with respect to Dyna-Q) are marked by &lt;code class=&quot;highlighter-rouge&quot;&gt;**********&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Initialize Q(s, a) for all (s, a) pairs with Q(terminal, .) = 0.
Initialize Tau(s, a) for all (s, a) pairs to be zero.  # **********
Initialize Model(s, a) for all (s, a) pairs to be (0, s).  # **********
Initialize States to be a set of all states  # **********
Initialize ActionsInState to be a dictionary mapping actions to states  # **********

Set max_episodes.
Set alpha - the learning rate.
Set gamma - the discount factor.
Set n - the planning-learning ratio.
Set k - the exploration constant.

for episode in range(max_episodes):

    Initialize s to be the starting state.
    
    Loop:
        
        for (s, a) in S cross A:  # **********
            Tau(s, a) += 1
        
        Choose a using s from the epsilon-greedy (behavior) policy derived from Q.
        Take action a, observe r and s'.

        # ===== learn model =====
        
        Tau(s, a) = 0  # **********
        Model[(s, a)] = (r, s')  # **********
            
        # ===== do learning =====

        Bellman sample = r + gamma * max_a Q(s', a)
        Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample

        s = s'
        
        # ===== do planning =====
        
        for _ in range(n):
            
            s = randomly chosen state from States
            a = random action chosen from ActionsInState[s]
            
            r, s' = Model[(s, a)]
            
            exploration bonus = k * sqrt(Tau(s, a))  # **********
            
            Bellman sample = (r + bonus) + gamma * max_a Q(s', a)  # **********
            Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample
        			
    until s is terminal.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry><entry><title type="html">TD Prediction</title><link href="http://localhost:4000/math/2020/07/27/td-prediction.html" rel="alternate" type="text/html" title="TD Prediction" /><published>2020-07-27T10:00:00-05:00</published><updated>2020-07-27T10:00:00-05:00</updated><id>http://localhost:4000/math/2020/07/27/td-prediction</id><content type="html" xml:base="http://localhost:4000/math/2020/07/27/td-prediction.html">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#on-policy-prediction&quot; id=&quot;markdown-toc-on-policy-prediction&quot;&gt;On-policy prediction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#off-policy-prediction&quot; id=&quot;markdown-toc-off-policy-prediction&quot;&gt;Off-policy prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;on-policy-prediction&quot;&gt;On-policy prediction&lt;/h2&gt;

&lt;p&gt;The Bellman equation for &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
v_{\pi}(s) &amp;= \sum_{a} \pi(a \mid s) q_{\pi}(s, a) \\
&amp;= \sum_{a} \pi(a \mid s)\sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi}(s') \right]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In TD, we make both the outer and inner expectation implicit through sampling. We simply estimate the value of &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; as the average of all boostrapped returns following visits to &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;. Note that each boostrapped return is a sample of &lt;script type=&quot;math/tex&quot;&gt;R + v_{\pi}(S')&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; is the reward of taking &lt;script type=&quot;math/tex&quot;&gt;\pi(s)&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;S’&lt;/script&gt; is the next state of taking &lt;script type=&quot;math/tex&quot;&gt;\pi(s)&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;. The online update rule is shown below. A relevant derivation can be found in the TD Control post.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(S) \leftarrow V(S) + \frac{1}{n} \left[ R + \gamma V(S') - V(S) \right]&lt;/script&gt;

&lt;p&gt;Since value estimates are more accurate over time, we would like to give recent updates more weight; the new online update rule is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(S) \leftarrow V(S) + \alpha \left[ R + \gamma V(S') - V(S) \right] = (1 - \alpha) V(S) + \alpha (R + \gamma V(S'))&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\alpha \in (0, 1]&lt;/script&gt; is a constant stepsize, unlike &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we arrive at the algorithm below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/28/rbXAlVI4tye1JKg.png&quot; alt=&quot;image-20200727113505484&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;off-policy-prediction&quot;&gt;Off-policy prediction&lt;/h2&gt;

&lt;p&gt;The idea behind off-policy methods is actually strikingly simple:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A behavior policy is used to meet all the states.&lt;/li&gt;
  &lt;li&gt;The update rule estimates the value function with respect to the target policy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following this idea, we can easily design an off-policy version of TD prediction:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input: an arbitrary target policy pi
Algorithm parameter: alpha in (0, 1]
Initialize V(s), for all s in S+, with V(terminal) = 0

Loop for each episode:
    b &amp;lt;- any policy with coverage of pi
    Loop for each step of episode:
        
        # the behavior policy is justed for visiting all states
        # we don't care about the returns
        
        A &amp;lt;- action given by b for S
        Take action A, observe S'
        
        # the target policy is used for actual value updates
        
        A* &amp;lt;- action given by pi for S
        Take action A*, observe R*, S*'
        
        V(S) &amp;lt;- V(S) + alpha[R* + gamma * V(S*') - V(S*)]
        
        S &amp;lt;- S'
        
    until S is terminal.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But this algorithm assumes that we can take an action in some state, and then immediately return to that state and take a different action. In general, this is not possible. To get rid of this assumption, we 1) use the reward and next state under the behavior policy in the value update formula but 2) correct that formula with a weighted importance sampling ratio to account for the fact that what we really want are samples under the target policy.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input: an arbitrary target policy pi
Algorithm parameter: alpha in (0, 1]
Initialize V(s), for all s in S+, with V(terminal) = 0
Initialize, for all s in S:
    Q(S) in R
    C(S) &amp;lt;- 0

Loop for each episode:
    b &amp;lt;- ay policy with coverage of pi
    Loop for each step of episode:
        
        A &amp;lt;- action given by b for S
        Take action A, observe R, S'
        
        W = pi(A|S) / b(A|S)  # importance sampling ratio
        C(S) &amp;lt;- C(S) + W
        V(S) &amp;lt;- V(S) + (W / C(S))[R + gamma * V(S') - V(S)]
        
        S &amp;lt;- S'
        
    until S is terminal.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The relevant derivation of these update rules can be found in section 5.6 of Sutton &amp;amp; Barto (2018), although there were derived in terms of Monte Carlo methods. Unfortunately, I don’t think these update rules take into account of the fact that recent samples are more “correct” than old samples.&lt;/p&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry><entry><title type="html">SARSA vs. Q-Learning</title><link href="http://localhost:4000/rl/sarsa_qlearning_comparison" rel="alternate" type="text/html" title="SARSA vs. Q-Learning" /><published>2020-07-25T18:00:00-05:00</published><updated>2020-07-25T18:00:00-05:00</updated><id>http://localhost:4000/rl/sarsa-qlearning_comparison</id><content type="html" xml:base="http://localhost:4000/rl/sarsa_qlearning_comparison">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#theory&quot; id=&quot;markdown-toc-theory&quot;&gt;Theory&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#value-iteration&quot; id=&quot;markdown-toc-value-iteration&quot;&gt;Value iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#asychronous-dynamic-programming&quot; id=&quot;markdown-toc-asychronous-dynamic-programming&quot;&gt;Asychronous dynamic programming&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#practice&quot; id=&quot;markdown-toc-practice&quot;&gt;Practice&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#incremental-implementation&quot; id=&quot;markdown-toc-incremental-implementation&quot;&gt;Incremental implementation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pseudocode-of-sarsa-and-q-learning&quot; id=&quot;markdown-toc-pseudocode-of-sarsa-and-q-learning&quot;&gt;Pseudocode of SARSA and Q-learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cliff-walking-experiment&quot; id=&quot;markdown-toc-cliff-walking-experiment&quot;&gt;Cliff-walking experiment&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#environment&quot; id=&quot;markdown-toc-environment&quot;&gt;Environment&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#online-performance&quot; id=&quot;markdown-toc-online-performance&quot;&gt;Online performance&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#in-depth-explanation-of-online-performance&quot; id=&quot;markdown-toc-in-depth-explanation-of-online-performance&quot;&gt;In-depth explanation of online performance&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;theory&quot;&gt;Theory&lt;/h1&gt;

&lt;p&gt;This post discusses the intuition of SARSA and Q-learning, which in my opinion is missing from Sutton &amp;amp; Barto (2018).&lt;/p&gt;

&lt;h2 id=&quot;value-iteration&quot;&gt;Value iteration&lt;/h2&gt;

&lt;p&gt;To motivate the TD control algorithm, it is convenient to begin with the value iteration algorithm (the extreme version of truncated policy iteration):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initialize &lt;script type=&quot;math/tex&quot;&gt;Q(s, a)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;(s, a)&lt;/script&gt; pairs arbitrarily except that &lt;script type=&quot;math/tex&quot;&gt;Q(\text{terminal}, \cdot) = 0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Loop through all &lt;script type=&quot;math/tex&quot;&gt;(s, a)&lt;/script&gt;’s:
    &lt;ul&gt;
      &lt;li&gt;Do &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a) \leftarrow \sum_{s’, r} p(s’, r \mid s, a) \left[ r + \sum_{a'} \pi(a' \mid s’) q_{\pi}(s’, a’) \right]&lt;/script&gt;.&lt;/li&gt;
      &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a)&lt;/script&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Until &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a)&lt;/script&gt; has converged to &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a)&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is stable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The value iteration algorithm makes the following assumptions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All the state-action pairs are known (otherwise we won’t be able to loop through them in an organized fashion).&lt;/li&gt;
  &lt;li&gt;All state-action pairs are updated with the same frequency.&lt;/li&gt;
  &lt;li&gt;The dynamics of the environment &lt;script type=&quot;math/tex&quot;&gt;p(s’, r \mid s, a)&lt;/script&gt; is known.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;asychronous-dynamic-programming&quot;&gt;Asychronous dynamic programming&lt;/h2&gt;

&lt;p&gt;The concept of asynchronous DP is the bridge from value iteration to one-step temporal-difference methods. From Sutton &amp;amp; Barto (2018):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set. These algorithms update the values of states in any order whatsoever, using whatever values of other states happen to be available. The values of some states may be updated several times before the values of others are updated once. To converge correctly, however, an asynchronous algorithm must continue to update the values of all the states: it can’t ignore any state after some point in the computation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that, instead of sweeping systematically, we can simply use an epsilon-greedy policy to “encounter” all state-action pairs and update their value as they are encountered. &lt;strong&gt;Assumption 1 and 2 are removed.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The update rule can be used to evaluate a target policy (which we shall denote by &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;) that is different from the behavior policy (which we shall denote by &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;). This is because the mere purpose of the behavior policy, just like systematic sweeping in value iteration, is to encounter all state-action pairs.&lt;/p&gt;

&lt;p&gt;Nevertheless, a behavior policy that assigns the most probability to actions that are also the most probable under the target policy would visit the state-action pairs that are more relevant to optimal behavior more often.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption 3 can be removed&lt;/strong&gt; by making both the outer (and inner expectation) of the update rule implicit through sampling:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Previously: &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a) \leftarrow \sum_{s’, r} p(s’, r \mid s, a) \left[ r + \sum_{a'} \pi(a' \mid s’) q_{\pi}(s’, a’) \right]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Now: &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a) \leftarrow \text{the average of all samples (with more weight given to recent ones)}&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;Each sample looks like &lt;script type=&quot;math/tex&quot;&gt;r + q_{\pi}(s’, \pi(s'))&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; is the actual reward received by taking &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s’&lt;/script&gt; is the actual next state reached by the policy by taking &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;. For convenience, let’s just call this the Bellman sample.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;b=\pi&lt;/script&gt;, we have SARSA. SARSA is on-policy because it is behaving according to and evaluating the same policy.&lt;/p&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is a greedy policy, we have Q-learning. Q-learning is off-policy because it evaluates a target policy that is different from the behavior policy used for acting.&lt;/p&gt;

&lt;p&gt;If the inner expectation is explicit, we have expected SARSA.&lt;/p&gt;

&lt;p&gt;The practical differences between SARSA and Q-learning will be addressed later in this post.&lt;/p&gt;

&lt;h1 id=&quot;practice&quot;&gt;Practice&lt;/h1&gt;

&lt;h2 id=&quot;incremental-implementation&quot;&gt;Incremental implementation&lt;/h2&gt;

&lt;p&gt;Before outlining the pseudocode of SARSA and Q-learning, we first consider how to update an average &lt;script type=&quot;math/tex&quot;&gt;A_{n+1}&lt;/script&gt; in an online fashion using an one-step-older average &lt;script type=&quot;math/tex&quot;&gt;A_n&lt;/script&gt; and a newly available sample &lt;script type=&quot;math/tex&quot;&gt;a_{n}&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
A_{n+1} &amp;= \frac{1}{n} \sum_{i=1}^n a_i \\
&amp;= \frac{1}{n} \left( a_n + \sum_{i=1}^{n-1} a_i \right) \\
&amp;= \frac{1}{n} \left( a_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} a_i \right) \\
&amp;= \frac{1}{n} \left( a_n + (n-1) A_{n} \right) \\
&amp;= \frac{1}{n} \left( a_n + nA_{n} - A_n \right) \\
&amp;= A_{n} + \frac{1}{n} \left[ a_n - A_n \right] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The assumption made by the incremental implementation above is that all samples are obtained from the distribution whose mean is what we want to estimate. Under the framework of general policy iteration, the distribution of returns is always changing since we alternate between evaluating the value function of the current policy and improving the current policy by setting the most probable action to be greedy to that value function. As a result, recent Bellman samples are more relevant than old ones. Therefore, we want to give newer updates more weight. This can be done by changing &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n}&lt;/script&gt; to a constant stepsize &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; that lies within the interval (0, 1]:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
A_{n+1} \leftarrow A_n + \alpha (a_n - A_n) 
\end{align*}&lt;/script&gt;

&lt;p&gt;Here’s a perhaps more intuitive way of rewriting this update rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
A_{n+1} \leftarrow (1 - \alpha) A_n + \alpha a_n
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;pseudocode-of-sarsa-and-q-learning&quot;&gt;Pseudocode of SARSA and Q-learning&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Initialize Q(s, a) for all (s, a) pairs with Q(terminal, .) = 0.
Set alpha.
Set mode to either SARSA or Q-learning.

Loop for each episode:
    Initialize s to be the starting state.
    Loop:
        
        Choose a from the epsilon-greedy (behavior) policy derived from Q.
        Take action a, observe s' and r.
            
        If mode is SARSA:
             Choose a' from the epsilon-greedy (target) policy derived from Q.
        If mode is Q-learning:
             Choose a' from the greedy (target) policy derived from Q.
            
        Bellman sample = r + Q(s', a')
        
        Q(s, a) = (1 - alpha) * Q(s, a) + alpha * Bellman sample

        s = s'
    
    until s is terminal.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cliff-walking-experiment&quot;&gt;Cliff-walking experiment&lt;/h2&gt;

&lt;h3 id=&quot;environment&quot;&gt;Environment&lt;/h3&gt;

&lt;p&gt;One way to understand the practical differences between SARSA and Q-learning is running them through a cliff-walking gridworld. For example, the following gridworld has 5 rows and 15 columns. Green regions represent walkable squares. Here’s the mapping from index to meaning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1: the starting state&lt;/li&gt;
  &lt;li&gt;4: the terminal state (stepping into this causes an epside to terminate; the policy is sent back to the starting state)&lt;/li&gt;
  &lt;li&gt;3: traps (stepping into a trap causes a -100 reward; the policy continue to take actions from this state though)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other than traps, stepping into a square incurs a reward of -1.&lt;/p&gt;

&lt;div style=&quot;max-width:600px; margin: auto&quot;&gt;
  &lt;img src=&quot;https://i.loli.net/2020/07/28/rvEBFeyNJ7onCKp.png&quot; style=&quot;max-width:100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;online-performance&quot;&gt;Online performance&lt;/h3&gt;

&lt;p&gt;Online performance is the performance of the behavior policy. Here are some important observations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Q-Learning’s best performance is optimal (the best possible) but SARSA’s best performance is not.&lt;/li&gt;
  &lt;li&gt;In terms of smoothed performance, SARSA is better than Q-learning most of the times.&lt;/li&gt;
  &lt;li&gt;SARSA occasionally have sudden drops in performance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, let’s try to understand the intuition behind these observations.&lt;/p&gt;

&lt;div style=&quot;max-width:800px; margin: auto&quot;&gt;
  &lt;img src=&quot;https://i.loli.net/2020/07/28/FfkQOgyP2HT6KCL.png&quot; style=&quot;max-width:100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;in-depth-explanation-of-online-performance&quot;&gt;In-depth explanation of online performance&lt;/h3&gt;

&lt;p&gt;Just like policy iteration / value iteration, Q-learning learns the value of the optimal greedy policy, which travels around the cliff as follows. However, during training (online), the total reward per episode is collected by the behavior policy, which is epsilon-greedy. When the behavior policy tries to walk around the cliff, its randomness can easily cause it to take an suboptimal action into a trap.&lt;/p&gt;

&lt;p&gt;Figure 1. The greedy trajectory learned by Q-learning.&lt;/p&gt;

&lt;p&gt;Legend:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Orange: grids that are on the learned greedy trajectory&lt;/li&gt;
  &lt;li&gt;Red: grids at which the learned greedy policy behaves sub-optimally (with respect to a policy-iteration baseline)&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;max-width:600px; margin: auto&quot;&gt;
  &lt;img src=&quot;https://i.loli.net/2020/07/29/yquiQ4wVZN6HvoX.jpg&quot; style=&quot;max-width:100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;On the other hand, SARSA takes this randomness into account because it learns the value of the behavior policy. Therefore, it uses extra 2 actions, one at the beginning of trajectory and another at the end, to exchange for a safer path.&lt;/p&gt;

&lt;p&gt;Figure 2. The greedy trajectory learned by SARSA.&lt;/p&gt;

&lt;div style=&quot;max-width:600px; margin: auto&quot;&gt;
  &lt;img src=&quot;https://i.loli.net/2020/07/29/3D8es7hqgyMBSC2.jpg&quot; style=&quot;max-width:100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Explanations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Q-learning’s best performance is optimal but SARSA’s best performance is not.
    &lt;ul&gt;
      &lt;li&gt;For reasons explained above, Q-learning learns the optimal greedy trajectory while SARSA doesn’t. Although Q-learning’s behavior policy is epsilon-greedy, while training, it is still possible for the behavior policy to always take greedy actions over an entire trajectory. When this happens, the total return of the trajectory is optimal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In terms of smoothed performance, SARSA is better than Q-learning most of the times.
    &lt;ul&gt;
      &lt;li&gt;Both Q-learning and SARSA uses the same epsilon-greedy behavior policy. However, the behavior policy of Q-learning walks closer to the traps and thus it is more likely to take an action into a trap due to randomness and receive a -100 reward.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SARSA occasionally have sudden drops in performance.
    &lt;ul&gt;
      &lt;li&gt;The behavior policy takes greedy actions most of the times and take random actions by 10% of the times.&lt;/li&gt;
      &lt;li&gt;This means that the state-action pairs on the greedy trajectory are encountered and evaluated more often; the number of evaluations for surrounding grids fall off exponentially as their distances to the greedy trajectory increase.&lt;/li&gt;
      &lt;li&gt;For SARSA, traps are 1 grid away from the greedy trajectory.&lt;/li&gt;
      &lt;li&gt;For Q-learning, traps are right next to the greedy trajectory.&lt;/li&gt;
      &lt;li&gt;For this reason, Q-learning evaluates actions in traps much frequently (Figure 3) and the greedy actions in all traps are upwards (also optimal) (Figure 1), which will lead the agent immediately out of the trap.&lt;/li&gt;
      &lt;li&gt;On the other hand, SARSA evaluates actions in traps only occasionally (Figure 4) and, even after 10000 episodes of training, the greedy actions for three traps are non-optimal* (Figure 1), which will lead the agent into another trap.&lt;/li&gt;
      &lt;li&gt;To put everything in a nutshell, although SARSA encounter less traps on average, when it does encounter one, it is more likely encounter several other traps because some actions in traps have incorrect values and thus the corresponding greedy actions are non-optimal*.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;*with respect to the environment and the epsilon-greedy behavior policy&lt;/p&gt;

&lt;p&gt;Figure 3: Number of evaluations for each state after running 10000 episodes of Q-learning.&lt;/p&gt;

&lt;div style=&quot;max-width:600px; margin: auto&quot;&gt;
  &lt;img src=&quot;https://i.loli.net/2020/07/28/dfqrea8DpKCjF5T.jpg&quot; style=&quot;max-width:100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Figure 4: Number of evaluations for each state after running 10000 episodes of SARSA.&lt;/p&gt;

&lt;div style=&quot;max-width:600px; margin: auto&quot;&gt;
  &lt;img src=&quot;https://i.loli.net/2020/07/28/81ghSFAaejqCvXc.jpg&quot; style=&quot;max-width:100%&quot; /&gt;
&lt;/div&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry><entry><title type="html">Policy Iteration in Practice</title><link href="http://localhost:4000/rl/policy_iteration_in_practice" rel="alternate" type="text/html" title="Policy Iteration in Practice" /><published>2020-07-21T11:30:00-05:00</published><updated>2020-07-21T11:30:00-05:00</updated><id>http://localhost:4000/rl/policy-iteration-in-practice</id><content type="html" xml:base="http://localhost:4000/rl/policy_iteration_in_practice">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#book-yes-but-use-either-discounting-or-non-deterministic-policy&quot; id=&quot;markdown-toc-book-yes-but-use-either-discounting-or-non-deterministic-policy&quot;&gt;Book: Yes, but use either discounting or non-deterministic policy.&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#environment&quot; id=&quot;markdown-toc-environment&quot;&gt;Environment&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#initial-q-table&quot; id=&quot;markdown-toc-initial-q-table&quot;&gt;Initial q-table&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-evaluation-update-formula&quot; id=&quot;markdown-toc-policy-evaluation-update-formula&quot;&gt;Policy-evaluation update formula&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#no-discounting-and-deterministic-policy&quot; id=&quot;markdown-toc-no-discounting-and-deterministic-policy&quot;&gt;No discounting and deterministic policy&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#discounting-and-deterministic-policy&quot; id=&quot;markdown-toc-discounting-and-deterministic-policy&quot;&gt;Discounting and deterministic policy&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#no-discounting-and-epsilon-greedy-policy&quot; id=&quot;markdown-toc-no-discounting-and-epsilon-greedy-policy&quot;&gt;No discounting and epsilon-greedy policy&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#me-i-would-like-to-learn-a-deterministic-policy-without-using-discounting&quot; id=&quot;markdown-toc-me-i-would-like-to-learn-a-deterministic-policy-without-using-discounting&quot;&gt;Me: I would like to learn a deterministic policy without using discounting.&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#approach-1-decay-epsilon-of-the-epsilon-greedy-policy&quot; id=&quot;markdown-toc-approach-1-decay-epsilon-of-the-epsilon-greedy-policy&quot;&gt;Approach 1: Decay epsilon of the epsilon-greedy policy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#approach-2-truncate-policy-evaluation&quot; id=&quot;markdown-toc-approach-2-truncate-policy-evaluation&quot;&gt;Approach 2: Truncate policy evaluation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gallery&quot; id=&quot;markdown-toc-gallery&quot;&gt;Gallery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Algorithms introduced in Chapter 5 (Monte-Carlo methods) and Chapter 6 (TD methods) of Sutton &amp;amp; Barto 2018 approximate the optimal q-table to achieve control. Since these methods were a bit hard to debug and only converge in the limit, I needed a tool to compute the exact optimal q-table to evaluate their performance. Therefore, I went back to Chapter 4 (dynamic programming) and implemented the policy iteration algorithm using q-tables with this question in mind:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Can policy iteration be used to learn optimal deterministic policies and their corresponding optimal value functions?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I also show some galleries of learned value functions in the end along with their source code.&lt;/p&gt;

&lt;h2 id=&quot;book-yes-but-use-either-discounting-or-non-deterministic-policy&quot;&gt;Book: Yes, but use either discounting or non-deterministic policy.&lt;/h2&gt;

&lt;p&gt;In many problems, our goal is to learn the optimal &lt;strong&gt;deterministic&lt;/strong&gt; policy. This is because deterministic policies perform optimally &lt;strong&gt;all the time&lt;/strong&gt; and thus obtain the highest reward.&lt;/p&gt;

&lt;p&gt;Policy iteration is an algorithm that allows us to find the optimal policy and the optimal value function in the limit. It consists of two sub-algorithms: policy evaluation and policy improvement. Certain requirements need to be met for policy evaluation to work. Here’s a relevant quote from section 4.1 of Sutton &amp;amp; Barto 2018:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The existence and uniqueness of &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}&lt;/script&gt; are guaranteed as long as either &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\gamma &lt; 1 %]]&gt;&lt;/script&gt; or eventual termination is guaranteed from all states under the policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To understand this quote, let’s consider an &lt;strong&gt;example&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;environment&quot;&gt;Environment&lt;/h3&gt;

&lt;p&gt;For simplicity, suppose there are only two states in a gridworld: one start state and one terminal state. The start state is on the left of the terminal state. Legal actions in non-terminal states (in this case, the start state only) are up, right, down and left and each action costs a reward of -1. After the agent arrives at the terminal state, the episode terminates and no further rewards are incurred. If the agent takes an action that makes itself outside of the gridworld, it is moved back to its last grid cell and receives a reward of -1.&lt;/p&gt;

&lt;h3 id=&quot;initial-q-table&quot;&gt;Initial q-table&lt;/h3&gt;

&lt;p&gt;To solve this problem, we store the value of each state-action pair (i.e., store the q-table) and do policy iteration.&lt;/p&gt;

&lt;p&gt;Suppose we use &lt;script type=&quot;math/tex&quot;&gt;\gamma = 1&lt;/script&gt; and the agent’s policy is deterministic. First, we randomly initialize the q-table, which is the initial uninformed estimate of the true q-table given the environment and the policy. Then we set the agent’s policy to be greedy with respect to this q-table.&lt;/p&gt;

&lt;p&gt;Suppose this q-table looks like this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;State&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Action&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Q-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;start state&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;up&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;start state&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;right&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-0.09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;start state&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;down&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.015&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;start state&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;left&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-0.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;policy-evaluation-update-formula&quot;&gt;Policy-evaluation update formula&lt;/h3&gt;

&lt;p&gt;First, we do policy evaluation. Note that the agent’s policy is NOT updated at all during this procedure. Let’s derive an update rule for policy evaluation. The relationship between &lt;script type=&quot;math/tex&quot;&gt;V_{\pi}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Q_\pi&lt;/script&gt; is listed below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_{\pi}(s) = \sum_{a} \pi(a \mid s) Q_{\pi}(s, a)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma V_{\pi}(s) \right]&lt;/script&gt;

&lt;p&gt;Since we store the q-table, we would like both the LHS and RHS to be expressed in terms of &lt;script type=&quot;math/tex&quot;&gt;Q_\pi&lt;/script&gt;. To do this, we substitue equation 1 into equation 2 and get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') Q_{\pi}(s', a') \right]&lt;/script&gt;

&lt;p&gt;which can be easily turned into the following update rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}(s, a) \leftarrow \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') Q_{\pi}(s', a') \right]&lt;/script&gt;

&lt;p&gt;We say policy evaluation has convergenced when the LHS and the RHS of the update rule above is the same. In other words, the stepsize of each update needs to converge to zero.&lt;/p&gt;

&lt;h4 id=&quot;no-discounting-and-deterministic-policy&quot;&gt;No discounting and deterministic policy&lt;/h4&gt;

&lt;p&gt;Since (state, up) has the highest initial value, the policy always instruct the agent to move up when the agent is in the start state. However,  moving up in start state, according to the rules of this environment, return the agent back to the start state and the story repeats for an infinite number of times. Therefore, the true value of (state, up) given the environment and the current policy is negative infinity.&lt;/p&gt;

&lt;p&gt;The corresponding update rule looks like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}(\text{start}, \uparrow) \leftarrow (-1) + Q_{\pi}(\text{start}, \uparrow)&lt;/script&gt;

&lt;p&gt;which clearly drives &lt;script type=&quot;math/tex&quot;&gt;Q_\pi(\text{start}, \uparrow)&lt;/script&gt; to negative infinity if repeated for an infinite number of times. However, in this case, we cannot check for convergence because the stepsize is always one.&lt;/p&gt;

&lt;h4 id=&quot;discounting-and-deterministic-policy&quot;&gt;Discounting and deterministic policy&lt;/h4&gt;

&lt;p&gt;Warning: This analysis here is not meant to be exhaustive. I haven’t looked at the formal proof of why policy evaluation requires either discounting or non-deterministic policy. Nevertheless, I hope they give you a good intuition on why things work / not work.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;true value&lt;/strong&gt; of &lt;script type=&quot;math/tex&quot;&gt;Q_\pi(\text{start}, \uparrow)&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;(-1) + \gamma(-1) + \gamma^2 (-1) + \cdots = \sum_{k=0}^{\infty} \gamma^k (-1) = \frac{(-1)}{1 - \gamma}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The corresponding update rule looks like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}(\text{start}, \uparrow) \leftarrow (-1) + \gamma Q_{\pi}(\text{start}, \uparrow)&lt;/script&gt;

&lt;p&gt;The first estimate is related to the initial estimate by the expression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}^1(\text{start}, \uparrow) = (-1) + \gamma Q_{\pi}^0(\text{start}, \uparrow)&lt;/script&gt;

&lt;p&gt;The second estimate is related to the initial estimate by the expression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Q_{\pi}^2(\text{start}, \uparrow) &amp;= (-1) + \gamma Q_{\pi}^1(\text{start}, \uparrow) \\
&amp;= (-1) + \gamma \left[ (-1) + \gamma Q_{\pi}^0(\text{start}, \uparrow) \right] \\
&amp;= (-1) + \gamma (-1) + \gamma^2 Q_{\pi}^0(\text{start}, \uparrow)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In general, the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-th estimate is related to the initial estimate by the expression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}^n(\text{start}, \uparrow) = \sum_{k=0}^{n-1} \gamma^{k} (-1) + \gamma^n Q_{\pi}^0(\text{start}, \uparrow)&lt;/script&gt;

&lt;p&gt;As &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; reaches infinity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}^{\infty}(\text{start}, \uparrow)  =\sum_{k=0}^{\infty} \gamma^{k}(-1) = \frac{-1}{1 - \gamma}&lt;/script&gt;

&lt;p&gt;We just showed that &lt;strong&gt;the estimated value converge to the true value in the limit&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now consider the &lt;strong&gt;upper bound&lt;/strong&gt; on the difference between the true value and the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-th estimated value:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\left\vert Q_{\pi}^{\infty}(\text{start}, \uparrow) - Q_{\pi}^n(\text{start}, \uparrow) \right\vert &amp;= \left\vert \sum_{k=0}^{\infty} \gamma^{k}(-1) - \sum_{k=0}^{n-1} \gamma^{k} (-1) - \gamma^n Q_{\pi}^0(\text{start}, \uparrow) \right\vert \\
&amp;= \left\vert \sum_{k=n}^{\infty} \gamma^{k}(-1) - \gamma^n Q_{\pi}^0(\text{start}, \uparrow) \right\vert \\
&amp;\leq \left\vert \sum_{k=n}^{\infty} \gamma^{k}(-1) \right\vert + \left\vert \gamma^n Q_{\pi}^0(\text{start}, \uparrow)  \right\vert
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;As &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; increases, this upper bound decreases at a decreasing rate (a bit mouthful but it’s true). This shows that, unlike in the last setting we considered, the stepsize decreases as we approach the goal. This allows us to stop policy iteration when the difference between the RHS and the LHS of the update rule reaches some preset precision tolerance.&lt;/p&gt;

&lt;p&gt;Hint for deriving the upper bound: &lt;script type=&quot;math/tex&quot;&gt;\left\vert a - b \right\vert = \left\vert a + (-b) \right\vert \leq \left\vert a \right\vert +  \left\vert -b \right\vert = \left\vert a \right\vert +  \left\vert b \right\vert&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;no-discounting-and-epsilon-greedy-policy&quot;&gt;No discounting and epsilon-greedy policy&lt;/h4&gt;

&lt;p&gt;The corresponding update rule looks like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{\pi}(\text{start}, \uparrow) \leftarrow (-1) + (1 - \epsilon)Q_{\pi}(\text{start}, \uparrow) + \frac{\epsilon}{\mid A \mid}  \left[ Q_{\pi}(\text{start}, \uparrow) +Q_{\pi}(\text{start}, \rightarrow) + Q_{\pi}(\text{start}, \downarrow) + Q_{\pi}(\text{start}, \rightarrow) \right]&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is the probability assigned to picking a random action (the greedy action can still be picked by chance).&lt;/p&gt;

&lt;p&gt;Note that I won’t go into any derivation here because the update rule is a lot more complicated, but the main ideas are still the same.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, the true value must be finite.&lt;/li&gt;
  &lt;li&gt;Second, the estimated value converges to the true value in the limit.&lt;/li&gt;
  &lt;li&gt;Third, the difference between the true value and estimated value decreases at a decreasing rate, so policy evaluation can be terminated early on after the difference between the RHS and the LHS of the update rule is lower than some precision tolerance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;me-i-would-like-to-learn-a-deterministic-policy-without-using-discounting&quot;&gt;Me: I would like to learn a deterministic policy without using discounting.&lt;/h2&gt;

&lt;p&gt;Personally, I find neither of these approaches satisfactory.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Discounting is mathematically convenient, but it seems to a bit artificial. For example, in the gridworld setting, without discounting, the value of state-action pair tells us the exact number of actions to take to get to terminal state, given that the reward of every action is -1; however, if we use discounting, this interpretation is not valid anymore.&lt;/li&gt;
  &lt;li&gt;Why learn a epsilon-greedy policy when we want a greedy policy more badly?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are some of my attempts that use &lt;script type=&quot;math/tex&quot;&gt;\gamma=1&lt;/script&gt; and learn a deterministic policy. Both of them worked in practice.&lt;/p&gt;

&lt;h3 id=&quot;approach-1-decay-epsilon-of-the-epsilon-greedy-policy&quot;&gt;Approach 1: Decay epsilon of the epsilon-greedy policy&lt;/h3&gt;

&lt;p&gt;Start with &lt;script type=&quot;math/tex&quot;&gt;\gamma=1&lt;/script&gt; and an epsilon-greedy policy. During each policy improvement step, decay epsilon by multiplying it by a constant like 0.8 or 0.9. Check for convergence to optimal value function and optimal policy using the Bellman’s optimality equation for epsilon-greedy policies. Eventually, epsilon becomes so small so that the epsilon-greedy policy is essentially a deterministic policy.&lt;/p&gt;

&lt;h3 id=&quot;approach-2-truncate-policy-evaluation&quot;&gt;Approach 2: Truncate policy evaluation&lt;/h3&gt;

&lt;p&gt;This really came as a surprise. When trying approach 1 for large gridworlds (10 by 10), I find that the first round policy evaluation takes forever, so I allowed a larger precision tolerance. The larger this tolerance was, the fast the algorithm ran. Then I thought, the use of very large policy-evaluation tolerance is essentially to truncated policy evaluation, which was mentioned briefly in the value iteration section of the book.&lt;/p&gt;

&lt;p&gt;Out of curiosity, I tried truncated policy evaluation starting from a greedy policy directly, and everything worked like a charm! This approach converged at least 5 times faster than approach 1. My implementation alternatives between truncated policy evaluation (one value update for all state-action pairs) + policy improvement.&lt;/p&gt;

&lt;p&gt;This might be why it works so well: truncated policy iteration relies on the fact that policy improvement can avoid a state-action pair as long as the value of that pair is lower than other pairs; it does not need to know that the return for that pair is negative infinity.&lt;/p&gt;

&lt;h2 id=&quot;gallery&quot;&gt;Gallery&lt;/h2&gt;

&lt;p&gt;Note the difference between  and &lt;code class=&quot;highlighter-rouge&quot;&gt;conv_tol&lt;/code&gt; in the following examples. &lt;code class=&quot;highlighter-rouge&quot;&gt;pe_tol&lt;/code&gt; is the precision tolerance for policy evaluation, while &lt;code class=&quot;highlighter-rouge&quot;&gt;conv_tol&lt;/code&gt; is the precision tolerance for convergence of policy iteration. &lt;code class=&quot;highlighter-rouge&quot;&gt;pe_tol&lt;/code&gt; is the maximum difference allowed between the LHS and the RHS of the update rule (which is really just the Bellman equation), while &lt;code class=&quot;highlighter-rouge&quot;&gt;conv_tol&lt;/code&gt; is the maximum difference allowed between the LHS and RHS of the Bellman optimality equation. More details are available from the &lt;a href=&quot;https://github.com/zhihanyang2022/classic_rl/blob/master/examples/policy_iteration/demo.ipynb&quot;&gt;notebook&lt;/a&gt; and the &lt;a href=&quot;https://zhihanyang2022.github.io/classic_rl/index.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Book’s first approach: discounting&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# very fast
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EpsilonGreedyPolicy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_initial_estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PolicyIteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;discount_factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;truncate_pe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pe_tol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conv_tol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-16&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/23/OX7LfYAsQFqES9J.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Book’s first approach: epsilon-greedy policy&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# very, very, very slow
# probably for the same reason mentioned in section &quot;No discounting and deterministic policy&quot;
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EpsilonGreedyPolicy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_initial_estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PolicyIteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;discount_factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;truncate_pe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pe_tol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conv_tol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-16&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It was so slow that I didn’t bother waiting for the results. I did verify that the policy evaluation error was decreasing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My second approach&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# very fast
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EpsilonGreedyPolicy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_initial_estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PolicyIteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;discount_factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;truncate_pe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pe_tol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conv_tol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-16&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/23/ROTDc34wMe6ZAhG.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s a more sophicated environment with walls and traps (large negative rewards of -5):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/23/almtEWnvpL3Frgh.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry><entry><title type="html">Sutton &amp;amp; Barto Chapter 5: Monte Carlo Methods</title><link href="http://localhost:4000/math/2020/07/10/c5-monte-carlo-methods.html" rel="alternate" type="text/html" title="Sutton &amp; Barto Chapter 5: Monte Carlo Methods" /><published>2020-07-10T10:30:00-05:00</published><updated>2020-07-10T10:30:00-05:00</updated><id>http://localhost:4000/math/2020/07/10/c5-monte-carlo-methods</id><content type="html" xml:base="http://localhost:4000/math/2020/07/10/c5-monte-carlo-methods.html">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#off-policy-prediction-via-importance-sampling&quot; id=&quot;markdown-toc-off-policy-prediction-via-importance-sampling&quot;&gt;Off-policy Prediction via Importance Sampling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#incremental-implementation&quot; id=&quot;markdown-toc-incremental-implementation&quot;&gt;Incremental implementation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#weighted-importance-sampling&quot; id=&quot;markdown-toc-weighted-importance-sampling&quot;&gt;Weighted importance sampling&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#state-value-function-off-policy-monte-carlo-prediction&quot; id=&quot;markdown-toc-state-value-function-off-policy-monte-carlo-prediction&quot;&gt;State-value function (off-policy Monte Carlo prediction)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#action-value-function-off-policy-monte-carlo-control&quot; id=&quot;markdown-toc-action-value-function-off-policy-monte-carlo-control&quot;&gt;Action-value function (off-policy Monte Carlo control)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;off-policy-prediction-via-importance-sampling&quot;&gt;Off-policy Prediction via Importance Sampling&lt;/h2&gt;

&lt;h2 id=&quot;incremental-implementation&quot;&gt;Incremental implementation&lt;/h2&gt;

&lt;h3 id=&quot;weighted-importance-sampling&quot;&gt;Weighted importance sampling&lt;/h3&gt;

&lt;h4 id=&quot;state-value-function-off-policy-monte-carlo-prediction&quot;&gt;State-value function (off-policy Monte Carlo prediction)&lt;/h4&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;G_1, G_2, \cdots, G_{n-1}&lt;/script&gt; are a sequence of returns starting in the same state &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;. Each &lt;script type=&quot;math/tex&quot;&gt;G_i&lt;/script&gt; is associated with a random weight &lt;script type=&quot;math/tex&quot;&gt;W_i&lt;/script&gt;. The weighted importance sampling method states that the estimate of &lt;script type=&quot;math/tex&quot;&gt;v(s)&lt;/script&gt; is given by:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align*}
V_n(s) = \frac{\sum_{k=1}^{n-1} W_k G_k}{\sum_{k=1}^{n-1}G_k}
\end{align*}&lt;/script&gt;
where &lt;script type=&quot;math/tex&quot;&gt;V_n(s)&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-th estimate of &lt;script type=&quot;math/tex&quot;&gt;v(s)&lt;/script&gt; that depends on the first &lt;script type=&quot;math/tex&quot;&gt;n-1&lt;/script&gt; returns and weights because &lt;script type=&quot;math/tex&quot;&gt;V_1(s)&lt;/script&gt; is set to be arbitrary and depends on no experiences with the environment. Since &lt;script type=&quot;math/tex&quot;&gt;V_n(s)&lt;/script&gt; has to depend on at least one return and one weight, &lt;script type=&quot;math/tex&quot;&gt;n \geq 2&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;n-1 \geq 1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;However, if we use the update rule above, then whenever we calculate &lt;script type=&quot;math/tex&quot;&gt;V_{n+1}(s)&lt;/script&gt;, we need access to &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; returns and weights, which can take up large amounts of memory when the state space &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}&lt;/script&gt; is large. Therefore, we are interested in an iterative update rule, where &lt;script type=&quot;math/tex&quot;&gt;V_{n+1}&lt;/script&gt; can be determined using &lt;script type=&quot;math/tex&quot;&gt;V_n(s)&lt;/script&gt;. &lt;strong&gt;Solution to exercise 5.10.&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
V_{n+1} &amp;= \frac{1}{\sum_{k=1}^nW_k} \sum_{k=1}^n W_k G_k\\
&amp;= \frac{1}{\sum_{k=1}^nW_k}  \left( W_n G_n + \sum_{k=1}^{n-1} W_k G_k \right) \\
&amp;= \frac{1}{\sum_{k=1}^nW_k} \left( W_n G_n + \left( \sum_{k=1}^{n}W_k - W_n \right) \left( \frac{1}{ \sum_{k=1}^{n}W_k - W_n} \right) \left(\sum_{k=1}^{n-1} W_k G_k \right) \right) \\
&amp;= \frac{1}{\sum_{k=1}^nW_k} \left( W_n G_n + \left( \sum_{k=1}^{n}W_k - W_n \right)V_n \right) \\
&amp;= \frac{1}{\sum_{k=1}^nW_k} \left( W_n G_n + V_n\sum_{k=1}^{n}W_k - V_nW_n \right) \\
&amp;= V_n + \frac{W_n}{\sum_{k=1}^nW_k} \left( G_n - V_n\right)
\end{align*} %]]&gt;&lt;/script&gt;
At this point, we have found an expression of &lt;script type=&quot;math/tex&quot;&gt;V_{n+1}&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;V_n&lt;/script&gt;, but the sum &lt;script type=&quot;math/tex&quot;&gt;\sum_{k=1}^n W_k&lt;/script&gt; still requires the memory of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;W_k&lt;/script&gt;’s. Therefore, we can let &lt;script type=&quot;math/tex&quot;&gt;C_{n-1} = \sum_{k=1}^{n-1} W_k&lt;/script&gt; and then &lt;script type=&quot;math/tex&quot;&gt;C_n = \sum_{k=1}^{n} W_k = W_n + \sum_{k=1}^{n-1} W_k = W_n + C_{n-1}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;**Note. ** &lt;script type=&quot;math/tex&quot;&gt;V_n&lt;/script&gt;: weighted average of &lt;script type=&quot;math/tex&quot;&gt;n - 1&lt;/script&gt; returns. &lt;script type=&quot;math/tex&quot;&gt;C_n&lt;/script&gt;: sum of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; weights. Both of them use the same subcript but they each denote a different number of terms. I think &lt;script type=&quot;math/tex&quot;&gt;C_n&lt;/script&gt; is used this way so that everything in the update rule has the same subscript. For example,&lt;/p&gt;

&lt;p&gt;For a state &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;, initialize &lt;script type=&quot;math/tex&quot;&gt;V_1(S)&lt;/script&gt; as arbitrary and &lt;script type=&quot;math/tex&quot;&gt;C_0(S)=0&lt;/script&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Collect a return starting from &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; and a weight&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;C_1 = W_1 + C_0&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;V_2 = V_1 + \frac{W_1}{C_1} (G_1 - V_1)&lt;/script&gt; (the update rule)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Conclusion.&lt;/strong&gt; Now organize the results of this derivation as the following algorithm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/11/eptx6IswSFAPcCg.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;action-value-function-off-policy-monte-carlo-control&quot;&gt;Action-value function (off-policy Monte Carlo control)&lt;/h4&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry><entry><title type="html">Sutton &amp;amp; Barto Chapter 4: Dynamic Programming</title><link href="http://localhost:4000/math/2020/07/04/c4-dynamic-programming.html" rel="alternate" type="text/html" title="Sutton &amp; Barto Chapter 4: Dynamic Programming" /><published>2020-07-04T15:22:00-05:00</published><updated>2020-07-04T15:22:00-05:00</updated><id>http://localhost:4000/math/2020/07/04/c4-dynamic-programming</id><content type="html" xml:base="http://localhost:4000/math/2020/07/04/c4-dynamic-programming.html">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#41-policy-evaluation&quot; id=&quot;markdown-toc-41-policy-evaluation&quot;&gt;4.1 Policy evaluation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#42-policy-improvement&quot; id=&quot;markdown-toc-42-policy-improvement&quot;&gt;4.2 Policy improvement&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-improvement-theorem&quot; id=&quot;markdown-toc-policy-improvement-theorem&quot;&gt;Policy improvement theorem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#43-policy-iteration&quot; id=&quot;markdown-toc-43-policy-iteration&quot;&gt;4.3 Policy iteration&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-iteration-using-the-state-value-function&quot; id=&quot;markdown-toc-policy-iteration-using-the-state-value-function&quot;&gt;Policy iteration using the state-value function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-iteration-using-the-action-value-function-answer-to-ex-45&quot; id=&quot;markdown-toc-policy-iteration-using-the-action-value-function-answer-to-ex-45&quot;&gt;Policy iteration using the action-value function (answer to Ex 4.5)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#44-value-iteration&quot; id=&quot;markdown-toc-44-value-iteration&quot;&gt;4.4 Value iteration&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#value-iteration-using-the-state-value-function&quot; id=&quot;markdown-toc-value-iteration-using-the-state-value-function&quot;&gt;Value iteration using the state-value function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#value-iteration-using-the-action-value-function-answer-to-ex-410&quot; id=&quot;markdown-toc-value-iteration-using-the-action-value-function-answer-to-ex-410&quot;&gt;Value iteration using the action-value function (answer to Ex 4.10)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#45-asychronous-dynamic-programming&quot; id=&quot;markdown-toc-45-asychronous-dynamic-programming&quot;&gt;4.5 Asychronous dynamic programming&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#46-generalize-policy-iteration&quot; id=&quot;markdown-toc-46-generalize-policy-iteration&quot;&gt;4.6 Generalize policy iteration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#47-efficiency-of-dynamic-programming&quot; id=&quot;markdown-toc-47-efficiency-of-dynamic-programming&quot;&gt;4.7 Efficiency of dynamic programming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;41-policy-evaluation&quot;&gt;4.1 Policy evaluation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/05/CjF68HbZAtlMmwD.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the updates done in DP algorithms are called &lt;strong&gt;expected updates&lt;/strong&gt; because they are based on an expectation over all possible next states rather than on a sample next state.&lt;/li&gt;
  &lt;li&gt;The algorithm above is the &lt;strong&gt;one-table implementation&lt;/strong&gt;; for the two-table version, we keep the new and old &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;’s as two tables. In practice, the one-table implementation usually converges faster because it uses new data as soon as they are available.&lt;/li&gt;
  &lt;li&gt;The iterative policy evaluation &lt;strong&gt;converges only in the limit&lt;/strong&gt;, so a termination condition is required.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;42-policy-improvement&quot;&gt;4.2 Policy improvement&lt;/h2&gt;

&lt;h3 id=&quot;policy-improvement-theorem&quot;&gt;Policy improvement theorem&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Given the state-value function of an arbitrary policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;, the new policy &lt;script type=&quot;math/tex&quot;&gt;\pi’&lt;/script&gt; obtained by acting greedy with respect to &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}&lt;/script&gt; is guaranteed to be better than &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;, unless &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is already optimal. In the later case, &lt;script type=&quot;math/tex&quot;&gt;\pi’&lt;/script&gt; would be optimal, too.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose we are in some arbitrary state &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;. The value of &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; under some arbitrary policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is given by &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}(s)&lt;/script&gt;. Define &lt;script type=&quot;math/tex&quot;&gt;\pi’(s)&lt;/script&gt; as the new policy that acts greedily with respect &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;, that is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\pi'(s) 
&amp;\triangleq \text{argmax}_{a} q_{\pi}(s, a) \\
&amp;= \text{argmax}_{a} \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi}(s') \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We want to show that &lt;script type=&quot;math/tex&quot;&gt;\pi’&lt;/script&gt; is guaranteed to be better than &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;, unless &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is already optimal.&lt;/p&gt;

&lt;p&gt;Because of how &lt;script type=&quot;math/tex&quot;&gt;\pi’&lt;/script&gt; is defined, it must be that &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, \pi’(s)) \geq q_{\pi}(s, \pi(s)) = v_{\pi}(s)&lt;/script&gt; because we have acted greedily. This is because &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is not necessarily greedy with respect to its own value function, which may be counter-intuitive. For an example of this, see Figure 4.1 on page 77.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; was chosen arbitrarily, we have &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, \pi’(s)) \geq v_{\pi}(s)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;. Then &lt;script type=&quot;math/tex&quot;&gt;v_{\pi'}(s) \geq v_{\pi}(s)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;  by the lemma below.&lt;/p&gt;

&lt;p&gt;Case 1. &lt;script type=&quot;math/tex&quot;&gt;v_{\pi’}(s) = v_{\pi}(s)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Then &lt;script type=&quot;math/tex&quot;&gt;v_{\pi’}(s) = \max_a \sum_{s, a} p(s’, r \mid s, a)\left[ r + \gamma v_{\pi}(s’) \right] \text{(iterative backup rule)}=\max_a \sum_{s, a} p(s’, r \mid s, a)\left[ r + \gamma v_{\pi'}(s’) \right]&lt;/script&gt;, which is the same the Bellman optimality equation.&lt;/li&gt;
  &lt;li&gt;Recall that the optimal value function is the unique solution to the Bellman optimality equation.&lt;/li&gt;
  &lt;li&gt;Therefore, &lt;script type=&quot;math/tex&quot;&gt;v_{\pi} = v_{\pi’} = v_{\ast}&lt;/script&gt; (the optimal state-value function) and &lt;script type=&quot;math/tex&quot;&gt;\pi=\pi’=\pi_{\ast}&lt;/script&gt; (the optimal policy).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Case 2. &lt;script type=&quot;math/tex&quot;&gt;\exists s \in \mathcal{S} (v_{\pi’}(s) &gt; v_{\pi}(s))&lt;/script&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi’&lt;/script&gt; is better than &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; because, for at least one &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;, it collects more return in expectation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; If &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, \pi’(s)) \geq v_{\pi}(s)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;,  then &lt;script type=&quot;math/tex&quot;&gt;v_{\pi'}(s) \geq v_{\pi}(s)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} 
v_{\pi}(s) 
&amp; \leq q_{\pi}\left(s, \pi^{\prime}(s)\right) \\ 
&amp;\stackrel{(1)}{=}\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=\pi^{\prime}(s)\right] \\
&amp;\stackrel{(2)}{=}\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
&amp; \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) \mid S_{t}=s\right] \\ 
&amp;\stackrel{(3)}{=}\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma \mathbb{E}_{\pi^{\prime}}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right) \mid S_{t+1} \right] \mid S_{t}=s\right] \\ 
&amp;\stackrel{(4)}{=}\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} v_{\pi}\left(S_{t+2}\right) \mid S_{t}=s\right] \\ 
&amp; \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} v_{\pi}\left(S_{t+3}\right) \mid S_{t}=s\right] \\ 
&amp; \vdots \\ 
&amp; \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots \mid S_{t}=s\right] \\ 
&amp;=v_{\pi^{\prime}}(s) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The way expectations are used here is quite peculiar and therefore worth discussion.
    &lt;ul&gt;
      &lt;li&gt;Conventionally, the expectation of a discrete random variable &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is denoted by &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{X \sim P} \left[ X \right]&lt;/script&gt; where the subscript of the expectation tells us how to look for the probability for each value &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; - simply using &lt;script type=&quot;math/tex&quot;&gt;P(x)&lt;/script&gt;.&lt;/li&gt;
      &lt;li&gt;Here, the subscripts merely contain the probability distributions (&lt;script type=&quot;math/tex&quot;&gt;\pi’(a \mid s)&lt;/script&gt; explicitly and the 4-argument &lt;script type=&quot;math/tex&quot;&gt;p(s’, r \mid s, a)&lt;/script&gt; implicitly) that are necessary to compute the probability distribution of the random variable of interest (which is kind of lazy but neat).
        &lt;ul&gt;
          &lt;li&gt;In equation 1, &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E} \left[\cdot \right]&lt;/script&gt; denotes the expected value of a random variable given the dynamics of the MDP.&lt;/li&gt;
          &lt;li&gt;In equation 2, &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\pi'} \left[\cdot \right]&lt;/script&gt; denotes the expected value of a random variable given the dynamics of the MDP and that the agent follows policy &lt;script type=&quot;math/tex&quot;&gt;\pi’&lt;/script&gt; in the current time-step.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Equation 3: just like what we did in equation 2 (the book made a mistake here but here it has been corrected).&lt;/li&gt;
  &lt;li&gt;Equation 4: it may not be super intuitive how this is obtained; here are some intermediate steps that make things easier.
    &lt;ul&gt;
      &lt;li&gt;Equation a: by linearity of expectations.&lt;/li&gt;
      &lt;li&gt;Equation b: by the law of total expectations (or Adam’s law).&lt;/li&gt;
      &lt;li&gt;Equation c: by linearity of expectations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma \mathbb{E}_{\pi^{\prime}}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right) \mid S_{t+1} \right] \mid S_{t}=s\right]

&amp;\stackrel{(a)}{=} \mathbb{E}_{\pi^{\prime}}\left[R_{t+1} \mid S_t=s\right] + \gamma \mathbb{E}_{\pi^{\prime}}\left[ \mathbb{E}_{\pi^{\prime}}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right) \mid S_{t+1} \right] \mid S_t=s \right]\\

&amp;\stackrel{(b)}{=} \mathbb{E}_{\pi^{\prime}}\left[R_{t+1} \mid S_t=s\right] + \gamma \mathbb{E}_{\pi^{\prime}}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right)\mid S_t=s \right]\\
&amp;\stackrel{(c)}{=}\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} v_{\pi}\left(S_{t+2}\right) \mid S_{t}=s\right] \\ 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;43-policy-iteration&quot;&gt;4.3 Policy iteration&lt;/h2&gt;

&lt;h3 id=&quot;policy-iteration-using-the-state-value-function&quot;&gt;Policy iteration using the state-value function&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/05/RgFlrpceTNsMqLU.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The section of the book that discusses the algorithm above is well-written:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/05/rzJukHWjoyFapUb.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the policy improvement step requires full knowledge of the MDP (the 4-argument p) and easy ways to access the possible next rewards and states. This is not required for policy iteration using the action-value function - that’s why algorithms like SARSA estimates the action-value function instead of the state-value function.&lt;/p&gt;

&lt;h3 id=&quot;policy-iteration-using-the-action-value-function-answer-to-ex-45&quot;&gt;Policy iteration using the action-value function (answer to Ex 4.5)&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{l}
\text { 1. Initialization } \\
\qquad Q(s, a) \in \mathbb{R}^2 \text { arbitrarily for all } (s, a) \in \mathcal{S} \times \mathcal{A}; \pi(s) \in \mathcal{A}(s) \text { arbitrarily for all } s \in \mathcal{S} \\
\text { 2. Policy Evaluation } \\ 
\qquad\text { Loop: } \\ 
\qquad\qquad
\begin{array}{l}
\Delta \leftarrow 0 \\ 
\text { Loop for each } (s, a) \in \mathcal{S} \times \mathcal{A} \text { : } \\
\qquad q \leftarrow Q(s, a) \\
\qquad Q(s, a) \leftarrow \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma Q\left(s^{\prime}, \pi(s')\right)\right] \\ 
\Delta \leftarrow \max (\Delta,|q-Q(s, a)|) \\
\text { until } \Delta&lt;\theta \text { (a small positive number determining the accuracy of estimation) }
\end{array} \\
% part 3: policy improvement
\begin{array}{l}
\text { 3. Policy Improvement } \\ 
\qquad
\begin{array}{l}
\text {policy-stable } \leftarrow \text { true } \\ 
\text {For each } s \in \mathcal{S} \text { : } \\ 
\qquad \begin{array}{l}
\text {old-action } \leftarrow \pi(s) \\
\pi(s) \leftarrow \arg \max _{a} Q(s, a)\\ 
\text {If old-action } \neq \pi(s) \text{ and } Q(s, \text{old-action}) \neq Q(s, \pi(s)), \text { then policy-stable } \leftarrow \text { false }
\end{array} \\
\text{If policy-stable, then stop and return } Q \approx q_{\ast} \text{ and } \pi \approx \pi_{\ast}; \text{else go to }2.
\end{array}
\end{array}
\end{array} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;44-value-iteration&quot;&gt;4.4 Value iteration&lt;/h2&gt;

&lt;h3 id=&quot;value-iteration-using-the-state-value-function&quot;&gt;Value iteration using the state-value function&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/07/05/u6XE8QrRiflypUz.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;value-iteration-using-the-action-value-function-answer-to-ex-410&quot;&gt;Value iteration using the action-value function (answer to Ex 4.10)&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{l}
\text {Algorithm parameter: a small threshold } \theta&gt;0 \text { determining accuracy of estimation } \\
\text {Initialize } Q(s, a), \text { for all } (s, a) \in \mathcal{S}^{+} \times \mathcal{A}, \text {arbitrarily except that } Q(\text {terminal}, a)=0 \text{ for all } a \text{ in } \mathcal{A}. \\ 
\text {Loop: } \\ 
\qquad \begin{array}{l}
\Delta \leftarrow 0 \\ 
\qquad \begin{array}{l}
\text {Loop for each } (s, a) \in \mathcal{S} \times \mathcal{A}: \\ 
q \leftarrow Q(s, a) \\ 
Q(s, a) \leftarrow \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \max_{a'} Q\left(s^{\prime}, a'\right)\right] \\
\end{array}\\
\end{array}\\
\text {until } \Delta&lt;\theta \\
\text {Output a deterministic policy, } \pi \approx \pi_{*}, \text { such that } \pi(s)=\arg \max _{a} Q(s, a).
\end{array} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;45-asychronous-dynamic-programming&quot;&gt;4.5 Asychronous dynamic programming&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The structure of updates is more flexible.&lt;/strong&gt; DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;These algorithms update the values of states in any order whatsoever, using whatever values of other states happen to be available.&lt;/li&gt;
  &lt;li&gt;The values of some states may be updated several times before the values of others are updated once.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Convergence.&lt;/strong&gt; To converge correctly, however, an asynchronous algorithm must continue to update the values of all the states: it can’t ignore any state after some point in the computation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The exact benefit of being flexible.&lt;/strong&gt; Avoiding sweeps does not necessarily mean that we can get away with less computation. It just means that an algorithm does not need to get locked into any hopelessly long sweep before it can make progress improving a policy.&lt;/p&gt;

&lt;p&gt;More specificially, asynchronous algorithms also make it easier to intermix computation with real-time interaction. The agent’s experience can be used to determine the states to which the DP algorithm applies its updates. At the same time, the latest value and policy information from the DP algorithm can guide the agent’s decision making.&lt;/p&gt;

&lt;p&gt;This makes it possible to &lt;u&gt;focus&lt;/u&gt; the DP algorithm’s updates onto parts of the state set that are most relevant to the agent, which is a repeated theme in reinforcement learning.&lt;/p&gt;

&lt;h2 id=&quot;46-generalize-policy-iteration&quot;&gt;4.6 Generalize policy iteration&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;What is GPI?&lt;/strong&gt; The term generalized policy iteration (GPI) refer to the general idea of letting policy-evaluation and policy evaluation improvement processes interact, independent of the granularity and other details of the two processes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Characteristics of GPI.&lt;/strong&gt; Most importantly, &lt;u&gt;almost all&lt;/u&gt; reinforcement learning methods are well described as GPI because they have the follow characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They have identiﬁable policies and value functions.&lt;/li&gt;
  &lt;li&gt;The policy is always being improved with respect to the value function.&lt;/li&gt;
  &lt;li&gt;The value function is always being driven toward the value function for the improved policy.&lt;/li&gt;
  &lt;li&gt;The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function - we arrive at the Bellman optimality equation and both the policy and the value function are optimal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theoretical value of GPI.&lt;/strong&gt; In some cases, GPI can be proved to converge, most notably for the classical DP methods that we have presented in this chapter. In other cases convergence has not been proved, but still the idea of GPI improves our understanding of the methods.&lt;/p&gt;

&lt;h2 id=&quot;47-efficiency-of-dynamic-programming&quot;&gt;4.7 Efficiency of dynamic programming&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Complexity.&lt;/strong&gt; Dynamic programming may not be practical for very large problems, but compared with other methods for solving MDPs, DP methods are actually quite efficient. If we ignore a few technical details, then the (worst case) time DP methods take to ﬁnd an optimal policy is &lt;u&gt;polynomial in the number of states and actions&lt;/u&gt;.&lt;/p&gt;

&lt;p&gt;If n and k denote the number of states and actions, this means that a DP method takes a number of computational operations that is less than some polynomial function of n and k.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The curse of dimensionality (large state sets).&lt;/strong&gt; Large state sets do create diffiulties, but these are inherent difficulties of the problem, not of DP as a solution method. In fact, DP is comparatively better suited to handling large state spaces than competing methods such as direct search and linear programming.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Asychronous methods for large state sets.&lt;/strong&gt; To complete even one sweep of a synchronous method requires computation and memory for every state. For some problems, even this much memory and computation is impractical, yet the problem is still potentially solvable because relatively few states occur along optimal solution trajectories.&lt;/p&gt;

&lt;p&gt;Asynchronous methods and other variations of GPI can be applied in such cases and may ﬁnd good or optimal policies much faster than synchronous methods can.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy iteration or value iteration?&lt;/strong&gt; In practice, standard DP methods can be used with today’s computers to &lt;u&gt;solve MDPs with millions of states&lt;/u&gt;. &lt;u&gt;Both policy iteration and value iteration are widely used&lt;/u&gt;, and it is not clear which, if either, is better in general. In practice, these methods usually converge much faster than their theoretical worst-case run times, particularly if they are started with good initial value functions or policies.&lt;/p&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry><entry><title type="html">Sutton &amp;amp; Barto Chapter 3: Finite Markov Decision Processes</title><link href="http://localhost:4000/math/2020/07/01/c3-finite-markov-decision-processes.html" rel="alternate" type="text/html" title="Sutton &amp; Barto Chapter 3: Finite Markov Decision Processes" /><published>2020-07-01T17:00:00-05:00</published><updated>2020-07-01T17:00:00-05:00</updated><id>http://localhost:4000/math/2020/07/01/c3-finite-markov-decision-processes</id><content type="html" xml:base="http://localhost:4000/math/2020/07/01/c3-finite-markov-decision-processes.html">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#31-agent-environment-interface&quot; id=&quot;markdown-toc-31-agent-environment-interface&quot;&gt;3.1 Agent-environment interface&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#32-goals-and-rewards&quot; id=&quot;markdown-toc-32-goals-and-rewards&quot;&gt;3.2 Goals and rewards&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#33-returns-and-episodes&quot; id=&quot;markdown-toc-33-returns-and-episodes&quot;&gt;3.3 Returns and episodes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#35-policies-and-value-functions&quot; id=&quot;markdown-toc-35-policies-and-value-functions&quot;&gt;3.5 Policies and value functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#36-optimal-policies-and-optimal-value-functions&quot; id=&quot;markdown-toc-36-optimal-policies-and-optimal-value-functions&quot;&gt;3.6 Optimal policies and optimal value functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;31-agent-environment-interface&quot;&gt;3.1 Agent-environment interface&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Dynamics of MDP.&lt;/strong&gt; Let &lt;script type=&quot;math/tex&quot;&gt;p:\mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to \left[0, 1\right]&lt;/script&gt; be defined as &lt;script type=&quot;math/tex&quot;&gt;p(s', r \mid s, a ) = \Pr \left\{ S_t=s', R_t=r \mid S_{t-1} = s, A_{t-1} =a \right\}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;32-goals-and-rewards&quot;&gt;3.2 Goals and rewards&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The reward hypothesis.&lt;/strong&gt; That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).&lt;/p&gt;

&lt;h2 id=&quot;33-returns-and-episodes&quot;&gt;3.3 Returns and episodes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Expected return (episodic tasks).&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;G_t = R_{t+1} + R_{t+2} + \cdots + R_{T}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expected return (continuing tasks).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are three equivalent expressions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1} = R_{t+1} + \gamma G_{t+1}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;If the reward received at each time-step is just one, then &lt;script type=&quot;math/tex&quot;&gt;G_t = \sum_{k=0}^{\infty}\gamma^k = \frac{1}{1 - \gamma}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;35-policies-and-value-functions&quot;&gt;3.5 Policies and value functions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Policy.&lt;/strong&gt; A function that maps from &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S} \times \mathcal{A}&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\left[0, 1\right]&lt;/script&gt; and is denoted by &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;State-value function.&lt;/strong&gt; The state-value function of a policy (denoted by &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}&lt;/script&gt;) maps each &lt;u&gt;state&lt;/u&gt; to the expected return of starting in that state and following &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
v_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t | S_t=s\right]
\end{align*}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Action-value function.&lt;/strong&gt; The action-value function of a policy (denoted by &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}&lt;/script&gt; maps each &lt;u&gt;state-action pair&lt;/u&gt; to the expected return of starting in that state, taking that action and then following &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t=s, A_t = a \right]
\end{align*}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Recursive definitions of &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}&lt;/script&gt;.&lt;/strong&gt; Discrete state, action and reward spaces are assumed for convenience. I’ve also included the cases of where &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is the optimal policy; in those cases, &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is replace with &lt;script type=&quot;math/tex&quot;&gt;\ast&lt;/script&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;TODO: mention the definition of the probability functions pi and 4-argument p&lt;/p&gt;

&lt;p&gt;Express &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}(s)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
v_{\pi}(s) &amp;= \mathbb{E}_{a \sim \pi(s)} \left[ q_{\pi}(S_t, A_t) \mid S_t = s\right] \\
&amp;= \sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Comment: &lt;script type=&quot;math/tex&quot;&gt;a \sim \pi(s)&lt;/script&gt; means the value of $A_t$ (denoted by $a$ by definition of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;) is drawn from a distribution specified by the value of &lt;script type=&quot;math/tex&quot;&gt;S_t&lt;/script&gt; (denoted by &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; by definition of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;). In other words, each &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is associated with a probability. This probability is necessary because &lt;script type=&quot;math/tex&quot;&gt;A_t&lt;/script&gt; is the only random variable inside the brackets (&lt;script type=&quot;math/tex&quot;&gt;S_t&lt;/script&gt; is used as a condition), and we need to know &lt;script type=&quot;math/tex&quot;&gt;p(a \mid s)&lt;/script&gt; (which is &lt;script type=&quot;math/tex&quot;&gt;\pi(a \mid s)&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;Express &lt;script type=&quot;math/tex&quot;&gt;v_{\ast}(s)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
v_{\ast}(s) &amp;= \max_{a \in \mathcal{A}} q_{\pi}(s, a) \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;p&gt;Express &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}(s)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
q_{\pi}(s, a) &amp;= \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_{t}=s,A_{t}=a\right] \\
&amp;= \sum_{s'\in \mathcal{S}, r \in \mathcal{R}} p(s’, r \mid s, a) \left[ r + \gamma v_{\pi}(s') \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Express &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;v_{\ast}(s)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
q_{\ast}(s, a) &amp;= \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma v_{\ast}(S_{t+1}) \mid  S_{t}=s, A_{t}=a\right] \\
&amp;= \sum_{s'\in \mathcal{S}, r \in \mathcal{R}} p(s’, r \mid s, a) \left[ r + \gamma v_{\ast}(s') \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;p&gt;By nesting the two identities above, we can easily demonstrate the following two identities.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Express &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}(s)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}(s)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
v_{\pi}(s) &amp;= \mathbb{E}_{a \sim \pi(s)} \left[ q_{\pi}(S_t, A_t) \mid S_t=t\right] \\
&amp;= \mathbb{E}_{a \sim \pi(s)} \left[ \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid A_t=a\right] \mid S_t = t\right]\\
&amp;= \sum_{a \in \mathcal{A}(s)} \pi(a \mid s) \sum_{s’ \in \mathcal{S}, r \in \mathcal{R}} p(s’, r \mid s, a)\left[ r + \gamma v_{\pi}(s’) \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Express  &lt;script type=&quot;math/tex&quot;&gt;v_{\ast}(s)&lt;/script&gt; in terms of  &lt;script type=&quot;math/tex&quot;&gt;v_{\ast}(s)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
v_{\ast}(s) &amp;= \max_{a \in \mathcal{A}} q_{\ast}(s, a) \\
&amp;= \max_{a \in \mathcal{A}} \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma v_{\ast}(S_{t+1}) \mid S_t=s, A_t=a\right]\\
&amp;= \max_{a \in \mathcal{A}} \sum_{s’ \in \mathcal{S}, r \in \mathcal{R}} p(s’, r \mid s, a)\left[ r + \gamma v_{\ast}(s’) \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;p&gt;Express &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s, a)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
q_{\pi}(s, a) &amp;= \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t=s, A_t=a\right] \\
&amp;= \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma \mathbb{E}_{a' \sim \pi(s')} \left[ q_{\pi}(S_{t+1},  A_{t+1}) \mid S_{t+1}=s'\right] \mid S_t=s, A_t=a \right] \\
&amp;= \sum_{s'\in \mathcal{S}, r \in \mathcal{R}} p(s’, r \mid s, a) \left[ r + \gamma \sum_{a' \in \mathcal{A}} \pi(a' \mid s') q_{\pi}(s', a') \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Express &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
q_{\ast}(s, a) &amp;= \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma v_{\ast}(S_{t+1}) \mid S_t=s, A_t=a\right] \\
&amp;= \mathbb{E}_{(s', r) \sim p(s', r|s, a)} \left[ R_{t+1} + \gamma \max_{a' \in \mathcal{A}} q_{\pi}(S_{t+1}, A_{t+1}) \mid S_t=s, A_t=a \right] \\
&amp;= \sum_{s'\in \mathcal{S}, r \in \mathcal{R}} p(s’, r \mid s, a) \left[ r + \gamma \max_{a' \in \mathcal{A}} q_{\pi}(s', a') \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;p&gt;All the identities above will prove to be very useful in understanding the motivation of deep RL algorithms.&lt;/p&gt;

&lt;h2 id=&quot;36-optimal-policies-and-optimal-value-functions&quot;&gt;3.6 Optimal policies and optimal value functions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Optimal state-value function.&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;v_{\ast}(s)=\max_{\pi} v_{\pi}(s)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimal action-value function.&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a) = \max_{\pi} q_{\pi}(s, a)&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;a \in \mathcal{A}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recusive definitions of &lt;script type=&quot;math/tex&quot;&gt;v_{\ast}(s)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a)&lt;/script&gt; in terms of &lt;script type=&quot;math/tex&quot;&gt;v_{\ast}(s)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;q_{\ast}(s, a)&lt;/script&gt;.&lt;/strong&gt; Shown in the previous section for convenience.&lt;/p&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry><entry><title type="html">Notes on Policy Gradient</title><link href="http://localhost:4000/math/2020/06/23/policy-gradient.html" rel="alternate" type="text/html" title="Notes on Policy Gradient" /><published>2020-06-23T23:02:25-05:00</published><updated>2020-06-23T23:02:25-05:00</updated><id>http://localhost:4000/math/2020/06/23/policy-gradient</id><content type="html" xml:base="http://localhost:4000/math/2020/06/23/policy-gradient.html">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#vanilla-policy-gradient&quot; id=&quot;markdown-toc-vanilla-policy-gradient&quot;&gt;Vanilla policy gradient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#comparison-to-supervised-learning&quot; id=&quot;markdown-toc-comparison-to-supervised-learning&quot;&gt;Comparison to supervised learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vanilla-reinforce&quot; id=&quot;markdown-toc-vanilla-reinforce&quot;&gt;Vanilla REINFORCE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#variants-of-vanilla-policy-gradient&quot; id=&quot;markdown-toc-variants-of-vanilla-policy-gradient&quot;&gt;Variants of vanilla policy gradient&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reward-to-go&quot; id=&quot;markdown-toc-reward-to-go&quot;&gt;Reward-to-go&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#simple-baseline-normalized-advantages&quot; id=&quot;markdown-toc-simple-baseline-normalized-advantages&quot;&gt;Simple baseline (normalized advantages)&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#intuition-of-subtracting-mean&quot; id=&quot;markdown-toc-intuition-of-subtracting-mean&quot;&gt;Intuition of subtracting mean&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#intuition-of-dividing-by-variance&quot; id=&quot;markdown-toc-intuition-of-dividing-by-variance&quot;&gt;Intuition of dividing by variance&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#analyze-variance&quot; id=&quot;markdown-toc-analyze-variance&quot;&gt;Analyze variance&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#optimal-baseline-minimize-variance&quot; id=&quot;markdown-toc-optimal-baseline-minimize-variance&quot;&gt;Optimal baseline (minimize variance)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#neural-network-baseline&quot; id=&quot;markdown-toc-neural-network-baseline&quot;&gt;Neural network baseline&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experimental-results&quot; id=&quot;markdown-toc-experimental-results&quot;&gt;Experimental results&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cumulative-reward-over-time&quot; id=&quot;markdown-toc-cumulative-reward-over-time&quot;&gt;Cumulative reward over time&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#trajectory-sampled-under-optimal-policy&quot; id=&quot;markdown-toc-trajectory-sampled-under-optimal-policy&quot;&gt;Trajectory sampled under optimal policy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;vanilla-policy-gradient&quot;&gt;Vanilla policy gradient&lt;/h2&gt;

&lt;p&gt;The policy gradient, just like any other gradient, point to the direction of fastest increase of some objective funcion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal.&lt;/strong&gt; We seek to maximize the following quantity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;J(\theta)\\ 
=&amp; \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ r(\tau) \right] \\
=&amp; \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=1}^T r(s_t, a_t) \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the parameters of a neural network.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\tau = \left\{s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_{T-1}, a_{T-1}, r_{T-1}, s_T \right\}&lt;/script&gt; is a &lt;strong&gt;finite&lt;/strong&gt; trajectory of alternating states, actions and rewards.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(\tau) = p(s_1) \prod_{t=1}^{T-1} \pi_\theta(a_t \mid s_t) p(s_{t+1} \mid s_t, a_t)&lt;/script&gt; is the probability of &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; under &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; is the reward function (built-in to the environment).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Idea.&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\theta_{\text{new}} = \theta_{\text{old}} + \alpha \nabla_{\theta} J(\theta)&lt;/script&gt;, now we need to consider how to evaluate &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta} J(\theta)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Derivation.&lt;/strong&gt; Here we derive an easy-to-evaluate form of &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta} J(\theta)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Identity 1: &lt;script type=&quot;math/tex&quot;&gt;\nabla_v f(v) = f(v) \frac{\nabla_v f(v)}{f(v)}=f(v) \nabla_v \log f(v)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla_{\theta} J(\theta) 
&amp;= \nabla_\theta \left\{ \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ r(\tau) \right] \right\} \\
&amp;= \nabla_\theta \left\{ \int \pi_\theta(\tau) r(\tau) d\tau \right\} \\
&amp;= \int \nabla_\theta \left\{ \pi_\theta(\tau) \right\} r(\tau) d\tau\\
&amp;= \int \pi_\theta(\tau) \nabla_\theta \left\{ \log\pi_\theta(\tau) \right\} r(\tau) d\tau \\
&amp;= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_\theta \left\{ \log \pi_\theta(\tau) \right\} r(\tau) \right] \\
&amp;= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_\theta \left\{ \log p(s_1) + \sum_{t=1}^{T-1} \log(\pi_\theta(a_t \mid s_t)) + \sum_{t=1}^{T-1} \log(p(s_{t+1} \mid s_t, a_t)) \right\} r(\tau) \right] \\
&amp;= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_\theta \left\{\sum_{t=1}^{T-1} \log(\pi_\theta(a_t \mid s_t))\right\} r(\tau) \right] \\
&amp;= 
\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ 
\underbrace{
\left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_t \mid s_t)) \right\} \right) 
}_{\text{gradient in favor of } \tau}
\underbrace{r(\tau)}_{\text{ reward of } \tau} 
\right] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In practice, this expectation can be evaluated by sampling trajectories using &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}&lt;/script&gt; (does not need to be optimal).&lt;/p&gt;

&lt;p&gt;We can interpret the purpose of this gradient as increasing the probability of high reward trajectories and decreasing the probability of low reward trajectories.&lt;/p&gt;

&lt;h2 id=&quot;comparison-to-supervised-learning&quot;&gt;Comparison to supervised learning&lt;/h2&gt;

&lt;p&gt;We seek to maximize the following quantity using the maximum likelihood approach:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;J_{\text{ML}}(\theta) \\
=&amp;\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\log \pi_{\theta} (\tau)\right] \\
=&amp;\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\sum_{t=1}^{T-1} \log \pi_{\theta} (a_t \mid s_t)\right] 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is what we did last time in behavior cloning.&lt;/p&gt;

&lt;p&gt;Note that the expectation is over trajectories sampled from the training distribution, not the on-policy distribution.&lt;/p&gt;

&lt;p&gt;The easy-to-evaluate form of its gradient can be derived as follows. The important thing to note that is we no longer need to differentiate the distribution of &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; w.r.t. &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; anymore.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla_{\theta} \left\{ J_{\text{ML}}(\theta) \right\} &amp;=\nabla_{\theta} \left\{ \mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\sum_{t=1}^{T-1} \log p_{\theta} (a_t \mid s_t)\right]  \right\} \\
&amp;= \nabla_{\theta} \left\{ \int p_{\text{train}}(\tau) \left[\sum_{t=1}^{T-1} \log p_{\theta} (a_t \mid s_t)\right] d\tau \right\} \\
&amp;= \int  p_{\text{train}}(\tau) \nabla_{\theta} \left\{ \sum_{t=1}^{T-1} \log p_{\theta} (a_t \mid s_t)\right\} d\tau\\
&amp;= \int  p_{\text{train}}(\tau) \left\{ \sum_{t=1}^{T-1}  \nabla_{\theta}\log p_{\theta} (a_t \mid s_t)\right\} d\tau\\
&amp;= \mathbb{E}_{\tau \sim p_{\text{train}}(\tau)} \left[ 
\underbrace{\sum_{t=1}^{T-1}  \nabla_{\theta}\log p_{\theta} (a_t \mid s_t)}_{\text{gradient in favor of }\tau} 
\right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The differences between behavior cloning and vanilla policy gradient are summarized below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Method&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Policy gradient&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Maximum likelihood (behavior cloning)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Function to maximize&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=1}^{T-1} r(s_t, a_t) \right]&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)}\left[\sum_{t=1}^{T-1} \log \pi_{\theta} (a_t, s_t)\right]&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Gradient&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \underbrace{\left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_t \mid s_t)) \right\} \right) }_{\text{gradient in favor of } \tau}\underbrace{r(\tau)}_{\text{ reward of } \tau} \right]&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\tau \sim p_{\text{train}}(\tau)} \left[ \sum_{t=1}^{T-1} \nabla_{\theta}\log p_{\theta} (a_t, s_t) \right]&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MC gradient estimate&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{N} \sum_{n=1}^N \left\{ \underbrace{\left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \right) }_{\text{gradient in favor of } \tau_n}\underbrace{r(\tau_n)}_{\text{ reward of } \tau_n} \right\}&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{N} \sum_{n=1}^N \left\{ \underbrace{\sum_{t=1}^{T-1} \nabla_{\theta}\log p_{\theta} (a_{n, t} \mid s_{n, t})}_{\text{gradient in favor of } \tau_n} \right\}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In English, &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta} J(\theta)&lt;/script&gt; weights the gradient in favor of &lt;script type=&quot;math/tex&quot;&gt;\tau_n&lt;/script&gt; by its reward, while &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}J_{\text{ML}}(\theta)&lt;/script&gt; weights all gradients equally.&lt;/li&gt;
  &lt;li&gt;The similarity between two gradients will help us compute the policy gradient.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vanilla-reinforce&quot;&gt;Vanilla REINFORCE&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Initialize &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Loop:
    &lt;ul&gt;
      &lt;li&gt;Sample set of &lt;script type=&quot;math/tex&quot;&gt;\tau_n&lt;/script&gt; by running &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(a_t \mid s_t)&lt;/script&gt; in some environment.&lt;/li&gt;
      &lt;li&gt;Compute &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}J(\theta)&lt;/script&gt; by using its MC gradient estimate rule in the table above.&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_{\text{new}} \leftarrow \theta_{\text{old}} + \alpha \nabla_{\theta}J(\theta)&lt;/script&gt;. (A more advanced optimizer can be used in practice.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variants-of-vanilla-policy-gradient&quot;&gt;Variants of vanilla policy gradient&lt;/h2&gt;

&lt;p&gt;Reference: answer by Jerry Liu: &lt;a href=&quot;https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance&quot;&gt;Why does the policy gradient method have a high variance?&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;reward-to-go&quot;&gt;Reward-to-go&lt;/h3&gt;

&lt;p&gt;Since we will be estimating policy gradients by sampling trajectories, the variance of the resulting gradients can be high. To reduce this variance, we need to eliminate as many random variables as possible from the MC gradient estimate formula. To do so, we first re-write the MC gradient estimate formula as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\nabla_{\theta}J(\theta) \\
\approx&amp; \frac{1}{N} \sum_{n=1}^N \left\{ \left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \right) \left( \sum_{t=1}^{T-1} r(s_{n, t}, a_{n, t}) \right)\right\} \\
=&amp; \frac{1}{N} \sum_{n=1}^N \left\{ \left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \left( \sum_{t'=1}^{T-1} r(s_{n, t'}, a_{n, t'}) \right)\right) \right\} \\
=&amp; \frac{1}{N} \sum_{n=1}^N \left\{ \left( \sum_{t=1}^{T-1} \nabla_\theta \left\{ \log(\pi_\theta(a_{n, t} \mid s_{n, t})) \right\} \left( \sum_{t'=t}^{T-1} r(s_{n, t'}, a_{n, t'}) \right)\right) \right\} \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where we exploited causality (future actions do not impact past rewards) in the last step to remove all &lt;script type=&quot;math/tex&quot;&gt;r(s_{n, t’}, a_{n, t’})&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
t’ &lt; t %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To see why each reward is a random variable, consider the reward of the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;-th state on the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th sampled trajectory. Obviously, this reward, &lt;script type=&quot;math/tex&quot;&gt;r(s_{i, k}, a_{i, k})&lt;/script&gt; can be a random variable depending on the outcomes of following random processes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Randomness in reward (aka. the reward function itself maybe stochastic).&lt;/li&gt;
  &lt;li&gt;Randomness in what &lt;script type=&quot;math/tex&quot;&gt;s_{i, k}&lt;/script&gt; is.&lt;/li&gt;
  &lt;li&gt;Randomness in what &lt;script type=&quot;math/tex&quot;&gt;a_{i, k}&lt;/script&gt; is given &lt;script type=&quot;math/tex&quot;&gt;s_{i, k}&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simple-baseline-normalized-advantages&quot;&gt;Simple baseline (normalized advantages)&lt;/h3&gt;

&lt;p&gt;The reason why this trick is called “normalized advantages” is because the scalar multiplier of each gradient term can be seen as its &lt;em&gt;advantage&lt;/em&gt;: if a gradient term has a larger multiplier relative to other gradient terms, a gradient-descent step benefits its objective function more than that of other gradient terms. What &lt;em&gt;normalized&lt;/em&gt; mean below is described below.&lt;/p&gt;

&lt;p&gt;Vanilla policy gradient + simple baseline:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{n=1}^N \left\{ \nabla_\theta \left\{ \log\pi_\theta(\tau_{n}) \right\} \left( \frac{r(\tau_n) - \mu}{\sigma}\right) \right\}
\end{align*}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; are the mean and standard deviation of &lt;script type=&quot;math/tex&quot;&gt;\left\{ r(\tau_n) \right\}&lt;/script&gt;. Doing so does not change the expected value of the gradient because:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\mathbb{E}\left[ \nabla_{\theta} \left\{ \log \pi_{\theta}(\tau) b \right\} \right] \\
=&amp; \int \pi_{\theta}(\tau) \nabla_{\theta} \left\{ \log \pi_{\theta}(\tau) b \right\} d\tau \\
=&amp; b \int \pi_{\theta}(\tau) \nabla_{\theta} \left\{ \log \pi_{\theta}(\tau) \right\} d\tau \\
=&amp; b \int \nabla_{\theta} \pi_{\theta}(\tau) d\tau \\
=&amp; b \nabla_{\theta} \int \pi_{\theta}(\tau) d\tau \\
=&amp; b \nabla_{\theta} 1 = 0
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In practice, it is often more convenient to compute the mean and standard deviation of &lt;script type=&quot;math/tex&quot;&gt;\left\{ r(a_{n, t} \mid s_{n, t}) \right\}&lt;/script&gt; and normalize these. For example, in reward-to-go, there is no such thing as “the reward of an entire trajectory” because the reward assigned to each state along a trajectory is different.&lt;/p&gt;

&lt;p&gt;In terms of performance, using this trick can drastically improve the stability of vanilla REINFORCE.&lt;/p&gt;

&lt;h4 id=&quot;intuition-of-subtracting-mean&quot;&gt;Intuition of subtracting mean&lt;/h4&gt;

&lt;p&gt;Suppose we sampled three trajectory and all of them yielded large positive rewards: 999, 1000, 1111. Then by the vanilla policy gradient, all three trajectories are made more likely. However, to further improve, we only care about trajectories that yield &lt;em&gt;higher-than-average&lt;/em&gt; rewards. Therefore, we subtract &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; from each reward. In this way, the updated rewards are: -1, 0, 1. In other words, the third trajectories is made more likely and the first trajectory is made less likely.&lt;/p&gt;

&lt;h4 id=&quot;intuition-of-dividing-by-variance&quot;&gt;Intuition of dividing by variance&lt;/h4&gt;

&lt;p&gt;Let’s consider a hypothetical environment. This environment is initialized such that it satisfies some condition &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. At each time-step, the agent interacts with the environment, receives an reward of 1 and observes the updated environment. This continues on until the updated environment no longer satisfies condition &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. We force the environment to terminate after 200 time-steps.&lt;/p&gt;

&lt;p&gt;Suppose we have a competent RL agent. Then its expected reward of each trajectory should look like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Early phase of learning: very small&lt;/li&gt;
  &lt;li&gt;Final phase of learning: close to 200 (200 time-steps in total; for each time-step, only consider the total reward in the future, according to reward-to-go)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To see why this can be a problem, consider the vanilla policy gradient:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{n=1}^N \left\{ \underbrace{\nabla_\theta \left\{ \log(\pi_\theta(\tau_{n}) \right\}}_{\text{gradient term}} \underbrace{r(\tau_n)}_{\text{scalar multiplier}} \right\}
\end{align*}&lt;/script&gt;

&lt;p&gt;Each gradient term gets multiplied by a small scalar in the early phase of learning but gets multiplied by a much large scalar later on. This might be very undesirable for gradient descent, and this problem will get worse if we put a large cap on episode length. Therefore, normalization of rewards can help gradient descent perform better, provided that a proper optimizer and a proper learning rate have been chosen.&lt;/p&gt;

&lt;h4 id=&quot;analyze-variance&quot;&gt;Analyze variance&lt;/h4&gt;

&lt;p&gt;For some trajectory &lt;script type=&quot;math/tex&quot;&gt;\tau_n&lt;/script&gt;, its contribution to the vanilla policy gradient &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}J(\theta)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\} r(\tau_n)&lt;/script&gt;. Since &lt;script type=&quot;math/tex&quot;&gt;\tau_n&lt;/script&gt; is the outcome of a random process and &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\} r(\tau_n)&lt;/script&gt; depends on this outcome, &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\} r(\tau_n)&lt;/script&gt; is the product of two random variables. The variance of two independent variables &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\mu_X^2 \sigma_Y^2 + \mu_Y^2 \sigma_X^2 + \sigma_X^2 \sigma_Y^2&lt;/script&gt;. Assuming that &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;r(\tau_n)&lt;/script&gt; are independent, the variance of their product can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
E[\nabla_{\theta} \left\{ \log\pi_\theta(\tau) \right\}]^2 \text{Var}[r(\tau)] + E[r(\tau)]^2 \text{Var}[\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}] + \text{Var}[\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}]\text{Var}[r(\tau)].
\end{align*}&lt;/script&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;r(\tau_n)&lt;/script&gt; is normalized (has a mean of zero and a variance of one), the variance of their product is now&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
E[\nabla_{\theta} \left\{ \log\pi_\theta(\tau) \right\}]^2 + 2 \text{Var}[\nabla_{\theta} \left\{ \log\pi_\theta(\tau_{n}) \right\}],
\end{align*}&lt;/script&gt;

&lt;p&gt;which is reduced. A more involved analysis can be used to select &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that not only reduces but also minimizes the variance.&lt;/p&gt;

&lt;h3 id=&quot;optimal-baseline-minimize-variance&quot;&gt;Optimal baseline (minimize variance)&lt;/h3&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h3 id=&quot;neural-network-baseline&quot;&gt;Neural network baseline&lt;/h3&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental results&lt;/h2&gt;

&lt;p&gt;Jupyter notebook is available &lt;a href=&quot;https://github.com/zhihanyang2022/deeprl_notes/blob/master/02_policy_gradient/cartpole_v0_policy_gradient_solver.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;cumulative-reward-over-time&quot;&gt;Cumulative reward over time&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zhihanyang2022/deeprl_notes_site/master/_posts/policy_gradient/learning_curve.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Legend:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cartpole_v0: &lt;a href=&quot;https://gym.openai.com/envs/CartPole-v0/&quot;&gt;CartPole-v0 (link to OpenAI’s gym page)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;pg: solved using policy gradient&lt;/li&gt;
  &lt;li&gt;50: trained for 50 iterations&lt;/li&gt;
  &lt;li&gt;100: for each iteration, 100 trajectories are sampled&lt;/li&gt;
  &lt;li&gt;rtg: reward-to-go&lt;/li&gt;
  &lt;li&gt;na: simple baseline (normalized advantages)&lt;/li&gt;
  &lt;li&gt;solid lines: median performance&lt;/li&gt;
  &lt;li&gt;shaped regions: 0.25 to 0.75 quantile performance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;na seems to determine the rate of convergence initially (na &lt;script type=&quot;math/tex&quot;&gt;\to&lt;/script&gt; high rate of convergence initially).&lt;/li&gt;
  &lt;li&gt;rtg seems to determine whether a variant actually converges (rtg &lt;script type=&quot;math/tex&quot;&gt;\to&lt;/script&gt; convergence eventually reached).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;trajectory-sampled-under-optimal-policy&quot;&gt;Trajectory sampled under optimal policy&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zhihanyang2022/deeprl_notes_site/master/_posts/policy_gradient/trajectory_under_learned_policy.png&quot; /&gt;&lt;/p&gt;</content><author><name>Zhihan Yang</name></author><summary type="html"></summary></entry></feed>