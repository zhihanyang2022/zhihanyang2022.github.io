---
layout: post
title:  "Scaling Beyond Masked Diffusion Language Models"
date:   2025-06-01 22:21:59 +00:00
image: /images/duo_scaling.png
categories: research
author: "Zhihan Yang"
authors: "Subham Sekhar Sahoo, Jean-Marie Lamercier*, <strong>Zhihan Yang*</strong>, Justin Deschenaux* (Joint Second Authors), Jingyu Liu, John Thickstun, Ante JukiÄ‡"
venue: "arXiv"
arxiv: https://arxiv.org/abs/2602.15014
code: https://github.com/s-sahoo/scaling-dllms
website: https://s-sahoo.com/scaling-dllms/
twitter: https://x.com/zhihanyang_/status/2023624944928633280
---
We demonstrate that uniform-state diffusion could beat masked diffusion on likelihood evaluation benchmarks and GSM8K. I led the full SFT pipeline for AR, MDLM, and Eso-LMs.

